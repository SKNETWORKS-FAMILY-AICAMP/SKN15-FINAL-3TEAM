# RunPod LLaMA ëª¨ë¸ ì„œë²„ ì™„ë²½ ì„¤ì • ê°€ì´ë“œ

ìƒˆë¡œìš´ RunPod Podì„ ìƒì„±í•˜ê³  Django ë°±ì—”ë“œì™€ ì—°ê²°í•˜ëŠ” ì „ì²´ ê³¼ì •

---

## ğŸ“‹ ëª©ì°¨

1. [RunPod Pod ìƒì„±](#1-runpod-pod-ìƒì„±)
2. [Hugging Face ì¸ì¦](#2-hugging-face-ì¸ì¦)
3. [ëª¨ë¸ ì„œë²„ ì„¤ì¹˜](#3-ëª¨ë¸-ì„œë²„-ì„¤ì¹˜)
4. [ì„œë²„ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸](#4-ì„œë²„-ì‹¤í–‰-ë°-í…ŒìŠ¤íŠ¸)
5. [Django ë°±ì—”ë“œ ì—°ê²°](#5-django-ë°±ì—”ë“œ-ì—°ê²°)
6. [ë¬¸ì œ í•´ê²°](#6-ë¬¸ì œ-í•´ê²°)

---

## 1. RunPod Pod ìƒì„±

### 1-1. RunPod ì›¹ì‚¬ì´íŠ¸ ì ‘ì†
- https://www.runpod.io/ ë¡œê·¸ì¸

### 1-2. Pod ìƒì„±
**Pods** â†’ **+ Deploy** í´ë¦­

**GPU ì„ íƒ:**
- RTX 3090 (24GB) ë˜ëŠ” RTX 4090 (24GB) ê¶Œì¥
- ìµœì†Œ: GPU 8GB VRAM

**í…œí”Œë¦¿ ì„ íƒ:**
- `PyTorch 2.1` ë˜ëŠ” `PyTorch 2.2+` ì„ íƒ
- CUDA 12.1 í¬í•¨ëœ í…œí”Œë¦¿

**ìŠ¤í† ë¦¬ì§€:**
- ìµœì†Œ 50GB ê¶Œì¥

**í¬íŠ¸ ì„¤ì •:**
- HTTP Port: `8000` ì¶”ê°€ (ì¤‘ìš”!)

**Deploy** í´ë¦­

### 1-3. Pod ì—°ê²° ì •ë³´ í™•ì¸
Podì´ ì‹œì‘ë˜ë©´ **Connect** ë²„íŠ¼ í´ë¦­

ì¤‘ìš” ì •ë³´:
- **SSH ëª…ë ¹ì–´**: `ssh root@xxx.proxy.runpod.net -p xxxxx`
- **HTTP Service URL**: `https://xxxxxxx-8000.proxy.runpod.net`

> ğŸ’¡ **HTTP Service URLì„ ë©”ëª¨ì¥ì— ë³µì‚¬í•´ë‘ì„¸ìš”!** (ë‚˜ì¤‘ì— Django ì„¤ì •ì— í•„ìš”)

---

## 2. Hugging Face ì¸ì¦

### 2-1. Hugging Face í† í° ìƒì„±
1. https://huggingface.co/settings/tokens ì ‘ì†
2. **New token** í´ë¦­
3. Token ì´ë¦„: `runpod_llama`
4. Type: **Read** ì„ íƒ
5. **Generate** í´ë¦­
6. í† í° ë³µì‚¬ (ì˜ˆ: `hf_xxxxxxxxxxxx`)

### 2-2. LLaMA 3.2 ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ ìš”ì²­
1. https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct ì ‘ì†
2. **Request access** í´ë¦­
3. Meta ë¼ì´ì„ ìŠ¤ ë™ì˜
4. ìŠ¹ì¸ ëŒ€ê¸° (ëª‡ ë¶„~ëª‡ ì‹œê°„)

> â³ **ìŠ¹ì¸ ëŒ€ê¸° ì¤‘**: Phi-3 ëª¨ë¸ì„ ì„ì‹œë¡œ ì‚¬ìš© ê°€ëŠ¥ (ì¸ì¦ ë¶ˆí•„ìš”)

---

## 3. ëª¨ë¸ ì„œë²„ ì„¤ì¹˜

### 3-1. RunPod SSH ì ‘ì†

ë¡œì»¬ í„°ë¯¸ë„ì—ì„œ:
```bash
ssh root@xxx.proxy.runpod.net -p xxxxx
```

### 3-2. ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±
```bash
cd /workspace
mkdir -p llama_server
cd llama_server
```

### 3-3. í•„ìš”í•œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ

**ë°©ë²• 1: ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (ê¶Œì¥)**
```bash
curl -O https://raw.githubusercontent.com/SKNETWORKS-FAMILY-AICAMP/SKN15-FINAL-3TEAM/main/runpod/runpod_setup.sh
bash runpod_setup.sh
```

**ë°©ë²• 2: ìˆ˜ë™ ì„¤ì¹˜**
```bash
# ì„œë²„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
curl -O https://raw.githubusercontent.com/SKNETWORKS-FAMILY-AICAMP/SKN15-FINAL-3TEAM/main/runpod/runpod_llama_server.py

# requirements ë‹¤ìš´ë¡œë“œ
curl -O https://raw.githubusercontent.com/SKNETWORKS-FAMILY-AICAMP/SKN15-FINAL-3TEAM/main/runpod/runpod_requirements.txt

# ì˜ì¡´ì„± ì„¤ì¹˜
pip install -r runpod_requirements.txt
```

### 3-4. Hugging Face ë¡œê·¸ì¸

```bash
pip install huggingface_hub
huggingface-cli login
```

í”„ë¡¬í”„íŠ¸ê°€ ë‚˜ì˜¤ë©´:
1. í† í° ì…ë ¥ (2-1ì—ì„œ ìƒì„±í•œ í† í°)
2. `Add token as git credential? (Y/n)` â†’ `Y` ì…ë ¥

---

## 4. ì„œë²„ ì‹¤í–‰ ë° í…ŒìŠ¤íŠ¸

### 4-1. í™˜ê²½ ë³€ìˆ˜ ì„¤ì •

**LLaMA 3.2 ì‚¬ìš© (ìŠ¹ì¸ ë°›ì€ ê²½ìš°):**
```bash
export MODEL_NAME="meta-llama/Llama-3.2-3B-Instruct"
export MAX_TOKENS=512
export TEMPERATURE=0.7
```

**Phi-3 ì‚¬ìš© (ìŠ¹ì¸ ëŒ€ê¸° ì¤‘):**
```bash
export MODEL_NAME="microsoft/Phi-3-mini-4k-instruct"
export MAX_TOKENS=512
export TEMPERATURE=0.7
```

### 4-2. ì„œë²„ ì‹¤í–‰

**í¬ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (í…ŒìŠ¤íŠ¸ìš©):**
```bash
python runpod_llama_server.py
```

ë‹¤ìŒ ë©”ì‹œì§€ê°€ ë‚˜ì˜¤ë©´ ì„±ê³µ:
```
âœ… ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ!
   - ëª¨ë¸: meta-llama/Llama-3.2-3B-Instruct
   - ë””ë°”ì´ìŠ¤: cuda
   - ìµœëŒ€ í† í°: 512
INFO:     Uvicorn running on http://0.0.0.0:8000
```

### 4-3. ì„œë²„ í…ŒìŠ¤íŠ¸ (ìƒˆ í„°ë¯¸ë„)

ìƒˆ SSH ì„¸ì…˜ì„ ì—´ì–´ì„œ:
```bash
curl http://localhost:8000/health
```

ì‘ë‹µ ì˜ˆì‹œ:
```json
{
  "status": "ok",
  "model_loaded": true,
  "model_name": "meta-llama/Llama-3.2-3B-Instruct",
  "device": "cuda",
  "gpu_available": true,
  "gpu_name": "NVIDIA GeForce RTX 3090"
}
```

### 4-4. ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (í”„ë¡œë•ì…˜)

í…ŒìŠ¤íŠ¸ ì„±ê³µ í›„:
```bash
# ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ
pkill -f python

# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup python runpod_llama_server.py > server.log 2>&1 &

# ë¡œê·¸ í™•ì¸
tail -f server.log
```

ì„œë²„ ì¤‘ì§€:
```bash
pkill -f runpod_llama_server
```

---

## 5. Django ë°±ì—”ë“œ ì—°ê²°

### 5-1. RunPod URL í™•ì¸

RunPod ëŒ€ì‹œë³´ë“œì—ì„œ:
1. **Pods** â†’ ì‹¤í–‰ ì¤‘ì¸ Pod ì„ íƒ
2. **Connect** â†’ **HTTP Services** â†’ **Port 8000**
3. URL ë³µì‚¬ (ì˜ˆ: `https://hrb45hj7po3w8b-8000.proxy.runpod.net`)

### 5-2. EC2 ì„œë²„ ì„¤ì •

EC2ì— SSH ì ‘ì†:
```bash
ssh ubuntu@3.37.175.204
```

**í™˜ê²½ ë³€ìˆ˜ ì„¤ì •:**
```bash
cd patent_backend
nano .env
```

`.env` íŒŒì¼ì— ì¶”ê°€:
```bash
# RunPod ëª¨ë¸ ì„œë²„ ì„¤ì •
MODEL_SERVER_URL=https://hrb45hj7po3w8b-8000.proxy.runpod.net
CHATBOT_SERVICE=llama

# CORS ì„¤ì • (ì´ë¯¸ ìˆìœ¼ë©´ ìˆ˜ì •)
CORS_ALLOWED_ORIGINS=http://localhost:3000,http://127.0.0.1:3000,http://3-bengio-veraclaim.com,https://3-bengio-veraclaim.com,http://52.79.153.131
```

ì €ì¥: `Ctrl+O` â†’ `Enter` â†’ `Ctrl+X`

### 5-3. ìµœì‹  ì½”ë“œ ê°€ì ¸ì˜¤ê¸°

```bash
cd patent_backend

# ë¡œì»¬ ë³€ê²½ì‚¬í•­ ë°±ì—…
git stash

# ìµœì‹  ì½”ë“œ ê°€ì ¸ì˜¤ê¸°
git pull origin main

# ë³€ê²½ì‚¬í•­ í™•ì¸
git diff HEAD~1 config/settings.py
```

### 5-4. Django ì„œë²„ ì¬ì‹œì‘

```bash
# ê¸°ì¡´ ì„œë²„ ì¢…ë£Œ
pkill -f "python manage.py runserver"

# ì„œë²„ ì¬ì‹œì‘
DJANGO_SETTINGS_MODULE=config.settings python3 manage.py runserver 0.0.0.0:8000
```

ë˜ëŠ” gunicorn ì‚¬ìš© ì‹œ:
```bash
sudo systemctl restart gunicorn
```

### 5-5. ì—°ê²° í…ŒìŠ¤íŠ¸

í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì±—ë´‡ í˜ì´ì§€ ì ‘ì† í›„ ë©”ì‹œì§€ ì „ì†¡:
- "ì•ˆë…•í•˜ì„¸ìš”"
- "ê²Œì„ íŠ¹í—ˆì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”"

---

## 6. ë¬¸ì œ í•´ê²°

### âŒ ë¬¸ì œ 1: PyTorch ë²„ì „ ì—ëŸ¬
```
ERROR: Could not find a version that satisfies the requirement torch==2.1.2
```

**í•´ê²°:**
```bash
# torchëŠ” RunPodì— ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŒ
# requirements.txtì—ì„œ torch ì¤„ì´ ì£¼ì„ ì²˜ë¦¬ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
grep torch runpod_requirements.txt

# ì¶œë ¥ ì˜ˆì‹œ:
# # torch>=2.1.0  # RunPod í…œí”Œë¦¿ì— ì´ë¯¸ ì„¤ì¹˜ë˜ì–´ ìˆìŒ
```

---

### âŒ ë¬¸ì œ 2: LLaMA ëª¨ë¸ ì ‘ê·¼ ê¶Œí•œ ì—ëŸ¬
```
OSError: You are trying to access a gated repo.
401 Client Error: Unauthorized
```

**ì›ì¸:** Hugging Face ì¸ì¦ ë˜ëŠ” LLaMA ìŠ¹ì¸ ë¯¸ì™„ë£Œ

**í•´ê²° ë°©ë²• 1: Hugging Face ì¬ë¡œê·¸ì¸**
```bash
huggingface-cli login
# í† í° ë‹¤ì‹œ ì…ë ¥
```

**í•´ê²° ë°©ë²• 2: Phi-3 ì„ì‹œ ì‚¬ìš©**
```bash
export MODEL_NAME="microsoft/Phi-3-mini-4k-instruct"
python runpod_llama_server.py
```

**í•´ê²° ë°©ë²• 3: ìŠ¹ì¸ ìƒíƒœ í™•ì¸**
- https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct ì ‘ì†
- "Request access" ë²„íŠ¼ì´ ë³´ì´ë©´ ì•„ì§ ë¯¸ìŠ¹ì¸
- "Files and versions" íƒ­ì´ ë³´ì´ë©´ ìŠ¹ì¸ ì™„ë£Œ

---

### âŒ ë¬¸ì œ 3: hf_transfer ì—ëŸ¬
```
ValueError: Fast download using 'hf_transfer' is enabled but 'hf_transfer' package is not available
```

**í•´ê²°:**
```bash
# ì˜µì…˜ 1: hf_transfer ì„¤ì¹˜
pip install hf_transfer

# ì˜µì…˜ 2: í™˜ê²½ ë³€ìˆ˜ë¡œ ë¹„í™œì„±í™”
export HF_HUB_ENABLE_HF_TRANSFER=0
python runpod_llama_server.py
```

---

### âŒ ë¬¸ì œ 4: CORS ì—ëŸ¬ (í”„ë¡ íŠ¸ì—”ë“œ)
```
Access to fetch at 'http://52.79.153.131/api/chatbot/send/' from origin 'http://3-bengio-veraclaim.com' has been blocked by CORS policy
```

**í•´ê²°:** EC2 ì„¤ì • í™•ì¸
```bash
# EC2ì—ì„œ
cd patent_backend
cat config/settings.py | grep CORS_ALLOWED_ORIGINS

# ë‹¤ìŒ ë„ë©”ì¸ë“¤ì´ í¬í•¨ë˜ì–´ ìˆì–´ì•¼ í•¨:
# http://3-bengio-veraclaim.com
# https://3-bengio-veraclaim.com
```

ì—†ìœ¼ë©´ .env íŒŒì¼ ìˆ˜ì •:
```bash
nano .env
# CORS_ALLOWED_ORIGINSì— ë„ë©”ì¸ ì¶”ê°€
```

---

### âŒ ë¬¸ì œ 5: ëª¨ë¸ ì„œë²„ ì—°ê²° ì‹¤íŒ¨
```
ëª¨ë¸ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.
```

**ì²´í¬ë¦¬ìŠ¤íŠ¸:**
1. **RunPod ì„œë²„ ì‹¤í–‰ ì¤‘?**
   ```bash
   # RunPodì—ì„œ
   ps aux | grep python
   ```

2. **RunPod URLì´ ì •í™•í•œê°€?**
   - RunPod ëŒ€ì‹œë³´ë“œì—ì„œ URL ì¬í™•ì¸
   - `.env` íŒŒì¼ì˜ MODEL_SERVER_URLê³¼ ë¹„êµ

3. **Djangoê°€ ì¬ì‹œì‘ë˜ì—ˆëŠ”ê°€?**
   ```bash
   # EC2ì—ì„œ
   pkill -f "python manage.py runserver"
   python3 manage.py runserver 0.0.0.0:8000
   ```

4. **RunPodì—ì„œ ì§ì ‘ í…ŒìŠ¤íŠ¸:**
   ```bash
   curl https://hrb45hj7po3w8b-8000.proxy.runpod.net/health
   ```

---

### âŒ ë¬¸ì œ 6: ì‘ë‹µì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ì´ìƒí•¨

**ì¦ìƒ:**
- "ì¹˜í‚¨ì´ ngoní•œ ê±°ì˜ˆìš”!" ê°™ì€ ì´ìƒí•œ ì‘ë‹µ
- 15~28 í† í°ë§Œ ìƒì„±

**ì›ì¸:** ì´ì „ ë²„ì „ì˜ ì„œë²„ ì½”ë“œ ì‚¬ìš© ì¤‘

**í•´ê²°:**
```bash
# RunPodì—ì„œ ì„œë²„ ì¤‘ì§€
pkill -f python

# ìµœì‹  íŒŒì¼ ë‹¤ìš´ë¡œë“œ
cd /workspace/llama_server
curl -H 'Cache-Control: no-cache' -o runpod_llama_server.py \
  "https://raw.githubusercontent.com/SKNETWORKS-FAMILY-AICAMP/SKN15-FINAL-3TEAM/main/runpod/runpod_llama_server.py?$(date +%s)"

# ì„œë²„ ì¬ì‹œì‘
export MODEL_NAME="meta-llama/Llama-3.2-3B-Instruct"
python runpod_llama_server.py
```

**ìµœì‹  ë²„ì „ íŠ¹ì§•:**
- `min_new_tokens=50`: ìµœì†Œ 50í† í° ìƒì„±
- `repetition_penalty=1.2`: ë°˜ë³µ ê°ì†Œ
- RAG ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
- ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

---

## ğŸ“Š ì„œë²„ ëª¨ë‹ˆí„°ë§

### ë¡œê·¸ ì‹¤ì‹œê°„ í™•ì¸
```bash
# RunPodì—ì„œ
tail -f server.log
```

ë¡œê·¸ ì˜ˆì‹œ:
```
2025-11-07 17:09:06 - INFO - ğŸ“ ìƒì„± ìš”ì²­: ì•ˆë…•í•˜ì„¸ìš”...
2025-11-07 17:09:06 - INFO -    - max_tokens: 512
2025-11-07 17:09:06 - INFO -    - temperature: 0.7
2025-11-07 17:09:06 - INFO -    - ëŒ€í™” íˆìŠ¤í† ë¦¬: 9ê°œ
2025-11-07 17:09:07 - INFO - âœ… ìƒì„± ì™„ë£Œ: 156 ê¸€ì, 87 í† í°
```

### GPU ì‚¬ìš©ëŸ‰ í™•ì¸
```bash
# RunPodì—ì„œ
nvidia-smi
```

### í”„ë¡œì„¸ìŠ¤ í™•ì¸
```bash
ps aux | grep python
```

---

## ğŸš€ ì™„ë£Œ!

ì¶•í•˜í•©ë‹ˆë‹¤! ì´ì œ ë‹¤ìŒ êµ¬ì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤:

```
í”„ë¡ íŠ¸ì—”ë“œ (Vercel)
    â†“
Django API (EC2)
    â†“
RunPod LLaMA ì„œë²„ (GPU)
```

**í…ŒìŠ¤íŠ¸:**
1. í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì±—ë´‡ ì ‘ì†
2. "ì•ˆë…•í•˜ì„¸ìš”" ì…ë ¥
3. AI ì‘ë‹µ í™•ì¸
4. "ë‚´ê°€ ë­ë¼ê³  ì¸ì‚¬í–ˆì–´?" ì…ë ¥ (ë©€í‹°í„´ í…ŒìŠ¤íŠ¸)
5. ì´ì „ ë‚´ìš©ì„ ê¸°ì–µí•˜ëŠ” ì‘ë‹µ í™•ì¸

---

## ğŸ“š ì°¸ê³  ë¬¸ì„œ

- [ì „ì²´ ë°°í¬ ê°€ì´ë“œ](./RUNPOD_DEPLOYMENT_GUIDE.md)
- [RunPod ê³µì‹ ë¬¸ì„œ](https://docs.runpod.io/)
- [LLaMA 3.2 ëª¨ë¸ í˜ì´ì§€](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)
- [Phi-3 ëª¨ë¸ í˜ì´ì§€](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)

---

## ğŸ’¡ íŒ

### Pod ë¹„ìš© ì ˆì•½
- ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ Pod **Stop** (ë¹„ìš© ì ˆê°)
- í•„ìš”í•  ë•Œë§Œ **Start**
- ë°ì´í„°ëŠ” ìœ ì§€ë¨

### ì•ˆì •ì ì¸ ìš´ì˜
- ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ ì‚¬ìš© (`nohup`)
- ë¡œê·¸ ì •ê¸° í™•ì¸ (`tail -f server.log`)
- GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ (`nvidia-smi`)

### ì„±ëŠ¥ ìµœì í™”
- `temperature`: 0.7 (ê¸°ë³¸ê°’, ê· í˜•)
  - ë‚®ì¶”ë©´ (0.3~0.5): ì¼ê´€ì , ë³´ìˆ˜ì  ì‘ë‹µ
  - ë†’ì´ë©´ (0.9~1.2): ì°½ì˜ì , ë‹¤ì–‘í•œ ì‘ë‹µ
- `max_tokens`: 512 (ê¸°ë³¸ê°’)
  - ë” ê¸´ ì‘ë‹µ í•„ìš” ì‹œ: 1024
- `min_new_tokens`: 50 (ì§§ì€ ì‘ë‹µ ë°©ì§€)

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸:** 2025-11-07
**ì»¤ë°‹:** e29cc2e (ë©€í‹°í„´ ì„±ëŠ¥ í–¥ìƒ ë²„ì „)
