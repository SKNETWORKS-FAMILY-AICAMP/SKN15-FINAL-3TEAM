"""
RunPodìš© Qwen 2.5 14B QLoRA ëª¨ë¸ ì„œë²„
íŠ¹í—ˆ ë¶„ì„ íŠ¹í™” ì±—ë´‡ ì„œë²„

ì„¤ì¹˜ í•„ìš”:
pip install fastapi uvicorn transformers peft accelerate bitsandbytes

ì‹¤í–‰:
python runpod_qwen_server.py

í™˜ê²½ ë³€ìˆ˜:
- MODEL_PATH: Qwen ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ê¸°ë³¸: /workspace/Qwen-14B-checkpoint-16)
- PORT: ì„œë²„ í¬íŠ¸ (ê¸°ë³¸: 8000)
"""

import os
import logging
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="Qwen Patent Analysis Server",
    description="Qwen 2.5 14B + QLoRA íŠ¹í—ˆ ë¶„ì„ ì „ë¬¸ ì„œë²„",
    version="2.0.0"
)

# CORS ì„¤ì • (Django ë°±ì—”ë“œ ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜: íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
device = None

# í™˜ê²½ ë³€ìˆ˜
MODEL_PATH = os.getenv("MODEL_PATH", "/workspace/Qwen-14B-checkpoint-16")
PORT = int(os.getenv("PORT", "8000"))


class GenerateRequest(BaseModel):
    """ìƒì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€", min_length=1)
    file_content: Optional[str] = Field(None, description="ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©")
    conversation_history: Optional[List[Dict]] = Field(
        None,
        description="ì´ì „ ëŒ€í™” ë‚´ì—­ [{'type': 'user'|'ai', 'content': '...'}]"
    )
    max_tokens: int = Field(
        512,
        description="ìµœëŒ€ ìƒì„± í† í° ìˆ˜",
        ge=1,
        le=2048
    )
    temperature: float = Field(
        0.7,
        description="ìƒì„± ì˜¨ë„ (0.0~2.0)",
        ge=0.0,
        le=2.0
    )


class GenerateResponse(BaseModel):
    """ìƒì„± ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    response: str = Field(..., description="AI ì‘ë‹µ í…ìŠ¤íŠ¸")
    model: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„")


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    status: str
    model: str
    model_loaded: bool
    device: str
    gpu_available: bool
    gpu_name: Optional[str] = None


def load_model():
    """Qwen ëª¨ë¸ + QLoRA ì–´ëŒ‘í„° ì´ˆê¸°í™”"""
    global model, tokenizer, device

    logger.info("=" * 60)
    logger.info("ğŸ”¥ Qwen 2.5 14B QLoRA ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì‹œì‘")
    logger.info("=" * 60)

    # GPU í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    if device == "cuda":
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… GPU: {gpu_name}")
        logger.info(f"âœ… GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
    else:
        logger.warning("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    try:
        # 1. ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        logger.info("ğŸ“¥ 1/3: ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© (Qwen 2.5 14B Instruct)...")
        base_model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen2.5-14B-Instruct",
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True,
            load_in_8bit=True if device == "cuda" else False  # 8ë¹„íŠ¸ ì–‘ìí™”
        )

        if device == "cpu":
            base_model = base_model.to(device)

        logger.info("âœ… ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # 2. QLoRA ì–´ëŒ‘í„° ì ìš©
        logger.info(f"ğŸ“¥ 2/3: QLoRA ì–´ëŒ‘í„° ë¡œë”© ({MODEL_PATH})...")
        model = PeftModel.from_pretrained(
            base_model,
            MODEL_PATH
        )
        logger.info("âœ… QLoRA ì–´ëŒ‘í„° ë¡œë”© ì™„ë£Œ")

        # 3. í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("ğŸ“¥ 3/3: í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_PATH,
            trust_remote_code=True
        )

        # pad_token ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        logger.info("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ í‰ê°€ ëª¨ë“œ
        model.eval()

        logger.info("=" * 60)
        logger.info("âœ… ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"   - ëª¨ë¸: Qwen 2.5 14B + QLoRA")
        logger.info(f"   - ë””ë°”ì´ìŠ¤: {device}")
        logger.info(f"   - ì²´í¬í¬ì¸íŠ¸: {MODEL_PATH}")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        raise


def build_qwen_prompt(
    message: str,
    file_content: Optional[str] = None,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    Qwen ChatML í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±

    Args:
        message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        file_content: ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©
        conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­

    Returns:
        Qwen ChatML í˜•ì‹ í”„ë¡¬í”„íŠ¸
    """
    # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ êµ¬ì„±
    messages = [
        {
            "role": "system",
            "content": (
                "ë‹¹ì‹ ì€ íŠ¹í—ˆ ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n"
                "**ì£¼ìš” ì—­í• :**\n"
                "1. íŠ¹í—ˆ ì²­êµ¬í•­ ë¶„ì„ ë° ì‹ ê·œì„±/ì§„ë³´ì„± í‰ê°€\n"
                "2. ìœ ì‚¬ íŠ¹í—ˆ ë¹„êµ ë¶„ì„\n"
                "3. íŠ¹í—ˆ ë“±ë¡ ê°€ëŠ¥ì„± íŒë‹¨ ë° ê°œì„  ì œì•ˆ\n"
                "4. ì¼ë°˜ì ì¸ ëŒ€í™” ë° ì§ˆì˜ì‘ë‹µ\n\n"
                "**ì‘ë‹µ ì›ì¹™:**\n"
                "- íŠ¹í—ˆ ì§ˆë¬¸: ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ ë¶„ì„ ì œê³µ\n"
                "- ì¼ë°˜ ëŒ€í™”: ì¹œì ˆí•˜ê³  ê°„ë‹¨í•œ ë‹µë³€\n"
                "- ë¶ˆí™•ì‹¤í•œ ë‚´ìš©: ì†”ì§í•˜ê²Œ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ê¸°\n"
                "- í•­ìƒ í•œêµ­ì–´ë¡œ ì‘ë‹µ"
            )
        }
    ]

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ìµœê·¼ 10í„´)
    if conversation_history:
        for hist in conversation_history[-10:]:
            role = "user" if hist.get('type') == 'user' else "assistant"
            content = hist.get('content', '')
            if content:
                messages.append({"role": role, "content": content})

    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€ êµ¬ì„±
    user_message_parts = []

    if file_content:
        user_message_parts.append(f"[ì²¨ë¶€ íŒŒì¼]\n{file_content[:1000]}\n")

    user_message_parts.append(message)

    messages.append({
        "role": "user",
        "content": "\n".join(user_message_parts)
    })

    # Qwen ChatML í˜•ì‹ìœ¼ë¡œ ìë™ ë³€í™˜
    prompt = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )

    return prompt


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()


@app.get("/", response_model=HealthResponse)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - í—¬ìŠ¤ì²´í¬"""
    gpu_name = None
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)

    return HealthResponse(
        status="ok",
        model="Qwen 2.5 14B + QLoRA (Patent Analysis)",
        model_loaded=model is not None,
        device=str(device),
        gpu_available=torch.cuda.is_available(),
        gpu_name=gpu_name
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return await root()


@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """
    Qwen ëª¨ë¸ë¡œ ì‘ë‹µ ìƒì„±

    ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ë©°, íŠ¹í—ˆ ë¶„ì„ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    """
    if model is None or tokenizer is None:
        raise HTTPException(
            status_code=503,
            detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )

    try:
        # 1. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_qwen_prompt(
            message=request.message,
            file_content=request.file_content,
            conversation_history=request.conversation_history
        )

        logger.info(f"ğŸ“ ìƒì„± ìš”ì²­: {request.message[:100]}...")
        logger.info(f"   - max_tokens: {request.max_tokens}")
        logger.info(f"   - temperature: {request.temperature}")
        logger.info(f"   - ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(request.conversation_history) if request.conversation_history else 0}í„´")

        # 2. í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=4096  # Qwen ìµœëŒ€ ê¸¸ì´
        ).to(device)

        # 3. ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                min_new_tokens=20,
                temperature=request.temperature,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.3,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3
            )

        # 4. ë””ì½”ë”©
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # 5. assistant ì‘ë‹µë§Œ ì¶”ì¶œ
        # Qwen ChatML í˜•ì‹: <|im_start|>assistant\nì‘ë‹µ<|im_end|>
        if "<|im_start|>assistant" in full_response:
            response_text = full_response.split("<|im_start|>assistant")[-1]
            response_text = response_text.replace("<|im_end|>", "").strip()
        else:
            # í´ë°±: í”„ë¡¬í”„íŠ¸ ì œê±°
            prompt_length = len(tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=True))
            response_text = full_response[prompt_length:].strip()

        # ë¹ˆ ì‘ë‹µ ë°©ì§€
        if not response_text:
            response_text = "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."

        logger.info(f"âœ… ìƒì„± ì™„ë£Œ - ì‘ë‹µ ê¸¸ì´: {len(response_text)} ê¸€ì")

        return GenerateResponse(
            response=response_text,
            model="Qwen 2.5 14B + QLoRA"
        )

    except torch.cuda.OutOfMemoryError:
        logger.error("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
        raise HTTPException(
            status_code=507,
            detail="GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. max_tokensë¥¼ ì¤„ì´ê±°ë‚˜ ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.get("/model-info")
async def model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if model is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    info = {
        "model_name": "Qwen 2.5 14B Instruct + QLoRA",
        "checkpoint_path": MODEL_PATH,
        "device": str(device),
        "specialization": "íŠ¹í—ˆ ë¶„ì„ (Patent Analysis)",
        "supported_languages": ["Korean", "English"],
    }

    if torch.cuda.is_available():
        info["gpu_name"] = torch.cuda.get_device_name(0)
        info["gpu_memory_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.1f} GB"
        info["gpu_memory_reserved"] = f"{torch.cuda.memory_reserved(0) / 1024**3:.1f} GB"

    return info


if __name__ == "__main__":
    import uvicorn

    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘ ì¤€ë¹„")
    logger.info(f"   - í˜¸ìŠ¤íŠ¸: 0.0.0.0")
    logger.info(f"   - í¬íŠ¸: {PORT}")
    logger.info(f"   - ëª¨ë¸ ê²½ë¡œ: {MODEL_PATH}")
    logger.info(f"ğŸ“– API ë¬¸ì„œ: http://0.0.0.0:{PORT}/docs")

    uvicorn.run(
        app,
        host="0.0.0.0",
        port=PORT,
        log_level="info",
        access_log=True
    )
