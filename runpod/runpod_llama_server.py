"""
RunPodìš© LLaMA ëª¨ë¸ ì„œë²„
GPU ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ë©° FastAPIë¡œ REST API ì œê³µ

RunPod ì„¤ì •:
1. Template: PyTorch 2.1 + CUDA 12.1
2. GPU: RTX 4090 (24GB) ì´ìƒ ê¶Œì¥
3. í¬íŠ¸: 8000 (HTTP)

í™˜ê²½ ë³€ìˆ˜:
- MODEL_NAME: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: microsoft/Phi-3-mini-4k-instruct)
- MAX_TOKENS: ìµœëŒ€ ìƒì„± í† í° (ê¸°ë³¸: 512)
- TEMPERATURE: ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0.7)

ì§€ì› ëª¨ë¸:
- microsoft/Phi-3-mini-4k-instruct (3.8B, ì¸ì¦ ë¶ˆí•„ìš”)
- meta-llama/Llama-3.2-3B-Instruct (ì¸ì¦ í•„ìš” - Hugging Face ë¡œê·¸ì¸ & ìŠ¹ì¸)

ì‹¤í–‰:
python runpod_llama_server.py
"""

import os
import logging
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="RunPod LLaMA Model Server",
    description="íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ì„ ìœ„í•œ LLaMA ëª¨ë¸ ì„œë²„",
    version="1.0.0"
)

# CORS ì„¤ì • (Django ë°±ì—”ë“œì—ì„œ ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
device = None

# í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥
MODEL_NAME = os.getenv("MODEL_NAME", "microsoft/Phi-3-mini-4k-instruct")
MAX_MODEL_TOKENS = int(os.getenv("MAX_TOKENS", "512"))
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))


class GenerateRequest(BaseModel):
    """ìƒì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€", min_length=1)
    file_content: Optional[str] = Field(None, description="ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©")
    conversation_history: Optional[List[Dict]] = Field(
        None,
        description="ì´ì „ ëŒ€í™” ë‚´ì—­ [{'type': 'user'|'ai', 'content': '...'}]"
    )
    max_tokens: int = Field(
        MAX_MODEL_TOKENS,
        description="ìµœëŒ€ ìƒì„± í† í° ìˆ˜",
        ge=1,
        le=2048
    )
    temperature: float = Field(
        DEFAULT_TEMPERATURE,
        description="ìƒì„± ì˜¨ë„ (0.0~2.0)",
        ge=0.0,
        le=2.0
    )


class GenerateResponse(BaseModel):
    """ìƒì„± ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    response: str = Field(..., description="AI ì‘ë‹µ í…ìŠ¤íŠ¸")
    tokens_used: int = Field(..., description="ì‚¬ìš©ëœ í† í° ìˆ˜")
    model: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„")


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    status: str
    model_loaded: bool
    model_name: str
    device: str
    gpu_available: bool
    gpu_name: Optional[str] = None


def load_model():
    """ëª¨ë¸ ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰)"""
    global model, tokenizer, device

    logger.info("=" * 60)
    logger.info("RunPod LLaMA ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì‹œì‘")
    logger.info("=" * 60)

    # GPU í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    if device == "cuda":
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… GPU: {gpu_name}")
        logger.info(f"âœ… GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
    else:
        logger.warning("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    # ëª¨ë¸ ë¡œë”©
    logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}")

    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("1/3: í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )

        # pad_tokenì´ ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        logger.info("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ ë¡œë“œ
        logger.info("2/3: ëª¨ë¸ ë¡œë”©... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        if device == "cpu":
            model = model.to(device)

        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ í‰ê°€ ëª¨ë“œ
        logger.info("3/3: ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì„¤ì •...")
        model.eval()

        logger.info("=" * 60)
        logger.info("âœ… ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"   - ëª¨ë¸: {MODEL_NAME}")
        logger.info(f"   - ë””ë°”ì´ìŠ¤: {device}")
        logger.info(f"   - ìµœëŒ€ í† í°: {MAX_MODEL_TOKENS}")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        raise


def build_llama_prompt(
    message: str,
    file_content: Optional[str] = None,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    LLaMA 3.2 Instruct í”„ë¡¬í”„íŠ¸ í˜•ì‹ ìƒì„±

    Args:
        message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        file_content: ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©
        conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­

    Returns:
        LLaMA í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
    """
    prompt_parts = ["<|begin_of_text|>"]

    # ì‹œìŠ¤í…œ ë©”ì‹œì§€
    system_message = (
        "ë‹¹ì‹ ì€ íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ì„ ë„ì™€ì£¼ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. "
        "íŠ¹í—ˆ ë¬¸ì„œ, ë…¼ë¬¸, ê¸°ìˆ  ë¶„ì„ ë“±ì— ëŒ€í•´ ì •í™•í•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. "
        "ë‹µë³€ì€ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ë©°, í•œêµ­ì–´ë¡œ ì‘ë‹µí•©ë‹ˆë‹¤."
    )

    prompt_parts.append("<|start_header_id|>system<|end_header_id|>")
    prompt_parts.append(f"{system_message}<|eot_id|>")

    # ì´ì „ ëŒ€í™” ë‚´ì—­ ì¶”ê°€ (ìµœê·¼ 10ê°œë§Œ)
    if conversation_history:
        for hist in conversation_history[-10:]:
            role = "user" if hist['type'] == 'user' else "assistant"
            prompt_parts.append(f"<|start_header_id|>{role}<|end_header_id|>")
            prompt_parts.append(f"{hist['content']}<|eot_id|>")

    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
    prompt_parts.append("<|start_header_id|>user<|end_header_id|>")

    # íŒŒì¼ ë‚´ìš©ì´ ìˆìœ¼ë©´ í¬í•¨
    if file_content:
        prompt_parts.append(f"[ì²¨ë¶€ íŒŒì¼ ë‚´ìš©]\n{file_content[:1000]}\n\n[ì§ˆë¬¸]\n{message}<|eot_id|>")
    else:
        prompt_parts.append(f"{message}<|eot_id|>")

    # AI ì‘ë‹µ ì‹œì‘
    prompt_parts.append("<|start_header_id|>assistant<|end_header_id|>")

    return "\n".join(prompt_parts)


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()


@app.get("/", response_model=HealthResponse)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - í—¬ìŠ¤ì²´í¬"""
    gpu_name = None
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)

    return HealthResponse(
        status="ok",
        model_loaded=model is not None,
        model_name=MODEL_NAME,
        device=str(device),
        gpu_available=torch.cuda.is_available(),
        gpu_name=gpu_name
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return await root()


@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """
    ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ëŠ” í…ìŠ¤íŠ¸ ìƒì„±

    íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ ì§ˆë¬¸ì— ëŒ€í•´ LLaMA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if model is None or tokenizer is None:
        raise HTTPException(
            status_code=503,
            detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )

    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_llama_prompt(
            message=request.message,
            file_content=request.file_content,
            conversation_history=request.conversation_history
        )

        logger.info(f"ğŸ“ ìƒì„± ìš”ì²­: {request.message[:100]}...")
        logger.info(f"   - max_tokens: {request.max_tokens}")
        logger.info(f"   - temperature: {request.temperature}")
        logger.info(f"   - ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(request.conversation_history) if request.conversation_history else 0}ê°œ")

        # í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048  # ì…ë ¥ ê¸¸ì´ ì œí•œ
        ).to(device)

        # ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # ë””ì½”ë”©
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)

        # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        # <|start_header_id|>assistant<|end_header_id|> ì´í›„ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ
        response_start = full_response.rfind("<|start_header_id|>assistant<|end_header_id|>")
        if response_start != -1:
            response_text = full_response[response_start + len("<|start_header_id|>assistant<|end_header_id|>"):]
            # <|eot_id|> ì œê±°
            response_text = response_text.replace("<|eot_id|>", "").strip()
            response_text = response_text.replace("<|end_of_text|>", "").strip()
        else:
            response_text = full_response

        # í† í° ìˆ˜ ê³„ì‚°
        tokens_used = outputs[0].shape[0] - inputs['input_ids'].shape[1]

        logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {len(response_text)} ê¸€ì, {tokens_used} í† í°")

        return GenerateResponse(
            response=response_text,
            tokens_used=tokens_used,
            model=MODEL_NAME
        )

    except torch.cuda.OutOfMemoryError:
        logger.error("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
        raise HTTPException(
            status_code=507,
            detail="GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. max_tokensë¥¼ ì¤„ì´ê±°ë‚˜ ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.get("/model-info")
async def model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if model is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    info = {
        "model_name": MODEL_NAME,
        "device": str(device),
        "max_tokens": MAX_MODEL_TOKENS,
        "default_temperature": DEFAULT_TEMPERATURE,
    }

    if torch.cuda.is_available():
        info["gpu_name"] = torch.cuda.get_device_name(0)
        info["gpu_memory_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.1f} GB"
        info["gpu_memory_reserved"] = f"{torch.cuda.memory_reserved(0) / 1024**3:.1f} GB"

    return info


if __name__ == "__main__":
    # ì„œë²„ ì‹¤í–‰
    port = int(os.getenv("PORT", "8000"))
    host = os.getenv("HOST", "0.0.0.0")

    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘: http://{host}:{port}")
    logger.info(f"ğŸ“– API ë¬¸ì„œ: http://{host}:{port}/docs")

    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True
    )
