"""
RunPodìš© LLaMA ëª¨ë¸ ì„œë²„
GPU ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ë©° FastAPIë¡œ REST API ì œê³µ

RunPod ì„¤ì •:
1. Template: PyTorch 2.1 + CUDA 12.1
2. GPU: RTX 4090 (24GB) ì´ìƒ ê¶Œì¥
3. í¬íŠ¸: 8000 (HTTP)

í™˜ê²½ ë³€ìˆ˜:
- MODEL_NAME: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: microsoft/Phi-3-mini-4k-instruct)
- MAX_TOKENS: ìµœëŒ€ ìƒì„± í† í° (ê¸°ë³¸: 512)
- TEMPERATURE: ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0.7)

ì§€ì› ëª¨ë¸:
- microsoft/Phi-3-mini-4k-instruct (3.8B, ì¸ì¦ ë¶ˆí•„ìš”)
- meta-llama/Llama-3.2-3B-Instruct (ì¸ì¦ í•„ìš” - Hugging Face ë¡œê·¸ì¸ & ìŠ¹ì¸)

ì‹¤í–‰:
python runpod_llama_server.py
"""

import os
import logging
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="RunPod LLaMA Model Server",
    description="íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ì„ ìœ„í•œ LLaMA ëª¨ë¸ ì„œë²„",
    version="1.0.0"
)

# CORS ì„¤ì • (Django ë°±ì—”ë“œì—ì„œ ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
device = None

# í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥
MODEL_NAME = os.getenv("MODEL_NAME", "microsoft/Phi-3-mini-4k-instruct")
MAX_MODEL_TOKENS = int(os.getenv("MAX_TOKENS", "512"))
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))


class GenerateRequest(BaseModel):
    """ìƒì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€", min_length=1)
    file_content: Optional[str] = Field(None, description="ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©")
    conversation_history: Optional[List[Dict]] = Field(
        None,
        description="ì´ì „ ëŒ€í™” ë‚´ì—­ [{'type': 'user'|'ai', 'content': '...'}]"
    )
    max_tokens: int = Field(
        MAX_MODEL_TOKENS,
        description="ìµœëŒ€ ìƒì„± í† í° ìˆ˜",
        ge=1,
        le=2048
    )
    temperature: float = Field(
        DEFAULT_TEMPERATURE,
        description="ìƒì„± ì˜¨ë„ (0.0~2.0)",
        ge=0.0,
        le=2.0
    )


class GenerateResponse(BaseModel):
    """ìƒì„± ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    response: str = Field(..., description="AI ì‘ë‹µ í…ìŠ¤íŠ¸")
    tokens_used: int = Field(..., description="ì‚¬ìš©ëœ í† í° ìˆ˜")
    model: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„")


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    status: str
    model_loaded: bool
    model_name: str
    device: str
    gpu_available: bool
    gpu_name: Optional[str] = None


def load_model():
    """ëª¨ë¸ ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰)"""
    global model, tokenizer, device

    logger.info("=" * 60)
    logger.info("RunPod LLaMA ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì‹œì‘")
    logger.info("=" * 60)

    # GPU í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    if device == "cuda":
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… GPU: {gpu_name}")
        logger.info(f"âœ… GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
    else:
        logger.warning("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    # ëª¨ë¸ ë¡œë”©
    logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}")

    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("1/3: í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )

        # pad_tokenì´ ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        logger.info("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ ë¡œë“œ
        logger.info("2/3: ëª¨ë¸ ë¡œë”©... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        if device == "cpu":
            model = model.to(device)

        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ í‰ê°€ ëª¨ë“œ
        logger.info("3/3: ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì„¤ì •...")
        model.eval()

        logger.info("=" * 60)
        logger.info("âœ… ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"   - ëª¨ë¸: {MODEL_NAME}")
        logger.info(f"   - ë””ë°”ì´ìŠ¤: {device}")
        logger.info(f"   - ìµœëŒ€ í† í°: {MAX_MODEL_TOKENS}")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        raise


def extract_explicit_memory(conversation_history: Optional[List[Dict]]) -> Dict[str, any]:
    """
    ëª…ì‹œì  ë©”ëª¨ë¦¬ ì¶”ì¶œ: ì¤‘ìš” ì •ë³´ë¥¼ Key-Value í˜•íƒœë¡œ ì €ì¥

    ë©€í‹°í„´ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ í™•ì‹¤í•œ ê¸°ì–µ ê´€ë¦¬
    """
    memory = {
        'facts': [],  # ì‚¬ìš©ìê°€ ë§í•œ ì‚¬ì‹¤ë“¤
        'preferences': [],  # ì‚¬ìš©ìì˜ ì„ í˜¸/ê³„íš
        'topics': set(),  # ëŒ€í™” ì£¼ì œ
        'last_mentioned': {}  # ê° ì‚¬ì‹¤ì´ ëª‡ í„´ ì „ì— ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€
    }

    if not conversation_history:
        return memory

    total_turns = len(conversation_history)

    for idx, msg in enumerate(conversation_history):
        if msg.get('type') != 'user':
            continue

        content = msg.get('content', '')
        turns_ago = total_turns - idx

        # íŒ¨í„´ ê¸°ë°˜ ì‚¬ì‹¤ ì¶”ì¶œ
        # "ë‚˜ëŠ” Xë‹¤" / "ë‚˜ X í• ê±°ì•¼" / "ë‚´ XëŠ” Yë‹¤"
        patterns = [
            ('ë¨¹ì„ê±°ì•¼', 'meal_plan'),
            ('ë¨¹ëŠ”ë‹¤', 'meal_plan'),
            ('ê°ˆê±°ì•¼', 'travel_plan'),
            ('ê°„ë‹¤', 'travel_plan'),
            ('ì¢‹ì•„í•´', 'preference'),
            ('ì‹«ì–´í•´', 'dislike'),
        ]

        for pattern, fact_type in patterns:
            if pattern in content:
                # í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (íŒ¨í„´ ì•ì˜ ëª…ì‚¬)
                words = content.split()
                for i, word in enumerate(words):
                    if pattern in word and i > 0:
                        key_info = ' '.join(words[max(0, i-3):i])
                        memory['facts'].append({
                            'type': fact_type,
                            'content': key_info + ' ' + pattern,
                            'turns_ago': turns_ago
                        })
                        memory['last_mentioned'][fact_type] = turns_ago

        # ì£¼ì œ ì¶”ì¶œ
        if 'íŠ¹í—ˆ' in content or 'ê²Œì„' in content:
            memory['topics'].add('íŠ¹í—ˆ')
        if 'ë…¼ë¬¸' in content or 'ì—°êµ¬' in content:
            memory['topics'].add('ë…¼ë¬¸')

    return memory


def build_structured_summary(memory: Dict, conversation_history: Optional[List[Dict]]) -> str:
    """
    êµ¬ì¡°í™”ëœ ëŒ€í™” ìš”ì•½ ìƒì„±

    ëª…í™•í•œ í˜•ì‹ìœ¼ë¡œ ì¤‘ìš” ì •ë³´ë¥¼ ì •ë¦¬í•˜ì—¬ ëª¨ë¸ì´ í™•ì‹¤í•˜ê²Œ ì´í•´í•˜ë„ë¡ í•¨
    """
    if not memory['facts'] and not memory['topics']:
        return ""

    summary_parts = ["[ëŒ€í™” ë©”ëª¨ë¦¬]"]

    # ì£¼ì œ
    if memory['topics']:
        summary_parts.append(f"ì£¼ì œ: {', '.join(memory['topics'])}")

    # ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì‚¬ì‹¤ë“¤ (ìµœê·¼ 5ê°œ)
    if memory['facts']:
        summary_parts.append("\ní•µì‹¬ ì‚¬ì‹¤:")
        for fact in memory['facts'][-5:]:
            summary_parts.append(
                f"  â€¢ {fact['content']} ({fact['turns_ago']}í„´ ì „)"
            )

    # ìµœê·¼ ê´€ë ¨ ëŒ€í™” (ì»¨í…ìŠ¤íŠ¸ìš©)
    if conversation_history and len(conversation_history) >= 2:
        recent = conversation_history[-2:]
        summary_parts.append("\nìµœê·¼ ëŒ€í™”:")
        for msg in recent:
            role = "ì‚¬ìš©ì" if msg['type'] == 'user' else "AI"
            content = msg['content'][:50] + "..." if len(msg['content']) > 50 else msg['content']
            summary_parts.append(f"  {role}: {content}")

    return "\n".join(summary_parts) + "\n\n"


def build_llama_prompt(
    message: str,
    file_content: Optional[str] = None,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    10í„´ ë‚´ í™•ì‹¤í•œ ê¸°ì–µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±

    ì „ëµ:
    1. ëª…ì‹œì  ë©”ëª¨ë¦¬: ì¤‘ìš” ì‚¬ì‹¤ì„ Key-Valueë¡œ ì¶”ì¶œ
    2. êµ¬ì¡°í™”ëœ ìš”ì•½: ëª…í™•í•œ í˜•ì‹ìœ¼ë¡œ ì •ë³´ ì •ë¦¬
    3. Few-Shot ì˜ˆì‹œ: ì˜¬ë°”ë¥¸ ê¸°ì–µ ë°©ë²• ì‹œë²”

    Args:
        message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        file_content: ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©
        conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­

    Returns:
        LLaMA í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
    """
    prompt_parts = ["<|begin_of_text|>"]

    # Few-Shot ì˜ˆì‹œê°€ í¬í•¨ëœ ì‹œìŠ¤í…œ ë©”ì‹œì§€ (í”„ë¡œì•¡í‹°ë¸Œ ê¸°ëŠ¥ ì¶”ê°€)
    system_message = (
        "ë‹¹ì‹ ì€ íŠ¹í—ˆ ë° ë…¼ë¬¸ ê²€ìƒ‰Â·ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n"
        "**ì¤‘ìš”: ëŒ€í™” ë‚´ìš©ì„ ì •í™•íˆ ê¸°ì–µí•˜ëŠ” ë°©ë²•**\n\n"
        "ì˜ˆì‹œ 1 - ì§ì ‘ ì§ˆë¬¸:\n"
        "ì‚¬ìš©ì: ë‚˜ ì˜¤ëŠ˜ ì¹˜í‚¨ ë¨¹ì„ê±°ì•¼\n"
        "AI: ì¹˜í‚¨ ë§›ìˆê²Œ ë“œì„¸ìš”\n"
        "ì‚¬ìš©ì: ë‚´ê°€ ë­ ë¨¹ëŠ”ë‹¤ê³  í–ˆì§€?\n"
        "âœ“ ì •ë‹µ: 'ì¹˜í‚¨ì„ ë¨¹ëŠ”ë‹¤ê³  í•˜ì…¨ìŠµë‹ˆë‹¤ (2í„´ ì „)'\n"
        "âœ— ì˜¤ë‹µ: 'ê¸°ì–µì´ ì•ˆ ë‚˜ìš”' / 'ë­”ê°€ ìŒì‹ì´ë¼ê³ ...'\n\n"
        "ì˜ˆì‹œ 2 - ì§ì ‘ ì§ˆë¬¸:\n"
        "ì‚¬ìš©ì: ë‚˜ ë‚´ì¼ ì„œìš¸ ê°„ë‹¤\n"
        "AI: ì¢‹ì€ ì—¬í–‰ ë˜ì„¸ìš”\n"
        "ì‚¬ìš©ì: ë‚´ê°€ ì–´ë”” ê°„ë‹¤ê³  í–ˆì–´?\n"
        "âœ“ ì •ë‹µ: 'ì„œìš¸ì— ê°„ë‹¤ê³  í•˜ì…¨ìŠµë‹ˆë‹¤ (2í„´ ì „)'\n\n"
        "ì˜ˆì‹œ 3 - í”„ë¡œì•¡í‹°ë¸Œ ì°¸ì¡°:\n"
        "ì‚¬ìš©ì: ë‚˜ Python ë°°ìš°ëŠ” ì¤‘ì´ì•¼\n"
        "AI: Python ë©‹ì§€ë„¤ìš”! ì–´ë–¤ í”„ë¡œì íŠ¸ í•˜ì‹¤ ê³„íšì´ì‹ ê°€ìš”?\n"
        "ì‚¬ìš©ì: ì›¹ ê°œë°œ í•˜ë ¤ê³ \n"
        "âœ“ ì •ë‹µ: 'ì¢‹ë„¤ìš”! Pythonìœ¼ë¡œ ì›¹ ê°œë°œ ì‹œì‘í•˜ì‹œëŠ”êµ°ìš”. Djangoë‚˜ Flask ê°™ì€ í”„ë ˆì„ì›Œí¬ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.'\n"
        "âœ— ì˜¤ë‹µ: 'ì›¹ ê°œë°œ ì¢‹ë„¤ìš”' (Python ë§¥ë½ ë¬´ì‹œ)\n\n"
        "ì˜ˆì‹œ 4 - í”„ë¡œì•¡í‹°ë¸Œ ì°¸ì¡°:\n"
        "ì‚¬ìš©ì: ë‚˜ ì˜¤ëŠ˜ ì¹˜í‚¨ ë¨¹ì„ê±°ì•¼\n"
        "AI: ì¹˜í‚¨ ë§›ìˆê² ë„¤ìš”!\n"
        "ì‚¬ìš©ì: ë°°ê³ íŒŒ\n"
        "âœ“ ì •ë‹µ: 'ì•„ê¹Œ ì¹˜í‚¨ ë“œì‹ ë‹¤ê³  í•˜ì…¨ëŠ”ë°, ì§€ê¸ˆ ì£¼ë¬¸í•˜ì‹œê² ì–´ìš”?'\n"
        "âœ— ì˜¤ë‹µ: 'ë­ ë“œì‹œê³  ì‹¶ìœ¼ì„¸ìš”?' (ì¹˜í‚¨ ë§¥ë½ ë¬´ì‹œ)\n\n"
        "**ì‘ë‹µ ê·œì¹™:**\n"
        "1. [ëŒ€í™” ë©”ëª¨ë¦¬] ì„¹ì…˜ì˜ ì •ë³´ë¥¼ ë°˜ë“œì‹œ ì°¸ì¡°í•˜ì„¸ìš”\n"
        "2. ê´€ë ¨ ì •ë³´ê°€ ìˆìœ¼ë©´ ìì—°ìŠ¤ëŸ½ê²Œ ì–¸ê¸‰í•˜ì„¸ìš” (ë¬¼ì–´ë³´ì§€ ì•Šì•„ë„!)\n"
        "3. ëª‡ í„´ ì „ì— ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ ì•Œë ¤ì£¼ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤\n"
        "4. ì •í™•í•œ ì •ë³´ë§Œ ë‹µë³€í•˜ê³ , ë¶ˆí™•ì‹¤í•˜ë©´ ì†”ì§íˆ ë§í•˜ì„¸ìš”\n"
        "5. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•˜ì„¸ìš”"
    )

    prompt_parts.append("<|start_header_id|>system<|end_header_id|>")
    prompt_parts.append(f"{system_message}<|eot_id|>")

    # ëª…ì‹œì  ë©”ëª¨ë¦¬ ì¶”ì¶œ + êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±
    memory = extract_explicit_memory(conversation_history)
    structured_summary = build_structured_summary(memory, conversation_history)

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ë©”ëª¨ë¦¬ê°€ ìˆìœ¼ë©´ ìµœì†Œí™”)
    max_history = 4 if structured_summary else 8

    if conversation_history:
        for hist in conversation_history[-max_history:]:
            role = "user" if hist['type'] == 'user' else "assistant"
            prompt_parts.append(f"<|start_header_id|>{role}<|end_header_id|>")
            prompt_parts.append(f"{hist['content']}<|eot_id|>")

    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
    prompt_parts.append("<|start_header_id|>user<|end_header_id|>")

    # ë©”ì‹œì§€ êµ¬ì„±
    user_message_parts = []

    # êµ¬ì¡°í™”ëœ ë©”ëª¨ë¦¬ (ê°€ì¥ ì¤‘ìš”!)
    if structured_summary:
        user_message_parts.append(structured_summary)

    # íŒŒì¼ ë‚´ìš©
    if file_content:
        user_message_parts.append(f"[ì²¨ë¶€ íŒŒì¼]\n{file_content[:1000]}\n")

    # ì‹¤ì œ ì§ˆë¬¸
    user_message_parts.append(f"[ì§ˆë¬¸]\n{message}")

    prompt_parts.append("\n".join(user_message_parts) + "<|eot_id|>")

    # AI ì‘ë‹µ ì‹œì‘
    prompt_parts.append("<|start_header_id|>assistant<|end_header_id|>")

    return "\n".join(prompt_parts)


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()


@app.get("/", response_model=HealthResponse)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - í—¬ìŠ¤ì²´í¬"""
    gpu_name = None
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)

    return HealthResponse(
        status="ok",
        model_loaded=model is not None,
        model_name=MODEL_NAME,
        device=str(device),
        gpu_available=torch.cuda.is_available(),
        gpu_name=gpu_name
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return await root()


@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """
    ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ëŠ” í…ìŠ¤íŠ¸ ìƒì„±

    íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ ì§ˆë¬¸ì— ëŒ€í•´ LLaMA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if model is None or tokenizer is None:
        raise HTTPException(
            status_code=503,
            detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )

    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_llama_prompt(
            message=request.message,
            file_content=request.file_content,
            conversation_history=request.conversation_history
        )

        logger.info(f"ğŸ“ ìƒì„± ìš”ì²­: {request.message[:100]}...")
        logger.info(f"   - max_tokens: {request.max_tokens}")
        logger.info(f"   - temperature: {request.temperature}")
        logger.info(f"   - ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(request.conversation_history) if request.conversation_history else 0}ê°œ")

        # í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048  # ì…ë ¥ ê¸¸ì´ ì œí•œ
        ).to(device)

        # ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                min_new_tokens=50,  # ìµœì†Œ 50í† í° ìƒì„±
                temperature=request.temperature,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.2,  # 1.1 â†’ 1.2 (ë°˜ë³µ ê°ì†Œ)
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3  # 3-gram ë°˜ë³µ ë°©ì§€
            )

        # ë””ì½”ë”©
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)

        # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        # <|start_header_id|>assistant<|end_header_id|> ì´í›„ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ
        response_start = full_response.rfind("<|start_header_id|>assistant<|end_header_id|>")
        if response_start != -1:
            response_text = full_response[response_start + len("<|start_header_id|>assistant<|end_header_id|>"):]
            # <|eot_id|> ì œê±°
            response_text = response_text.replace("<|eot_id|>", "").strip()
            response_text = response_text.replace("<|end_of_text|>", "").strip()
        else:
            response_text = full_response

        # í† í° ìˆ˜ ê³„ì‚°
        tokens_used = outputs[0].shape[0] - inputs['input_ids'].shape[1]

        logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {len(response_text)} ê¸€ì, {tokens_used} í† í°")

        return GenerateResponse(
            response=response_text,
            tokens_used=tokens_used,
            model=MODEL_NAME
        )

    except torch.cuda.OutOfMemoryError:
        logger.error("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
        raise HTTPException(
            status_code=507,
            detail="GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. max_tokensë¥¼ ì¤„ì´ê±°ë‚˜ ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.get("/model-info")
async def model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if model is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    info = {
        "model_name": MODEL_NAME,
        "device": str(device),
        "max_tokens": MAX_MODEL_TOKENS,
        "default_temperature": DEFAULT_TEMPERATURE,
    }

    if torch.cuda.is_available():
        info["gpu_name"] = torch.cuda.get_device_name(0)
        info["gpu_memory_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.1f} GB"
        info["gpu_memory_reserved"] = f"{torch.cuda.memory_reserved(0) / 1024**3:.1f} GB"

    return info


if __name__ == "__main__":
    # ì„œë²„ ì‹¤í–‰
    port = int(os.getenv("PORT", "8000"))
    host = os.getenv("HOST", "0.0.0.0")

    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘: http://{host}:{port}")
    logger.info(f"ğŸ“– API ë¬¸ì„œ: http://{host}:{port}/docs")

    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True
    )
