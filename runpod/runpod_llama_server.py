"""
RunPodìš© LLaMA ëª¨ë¸ ì„œë²„
GPU ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ë©° FastAPIë¡œ REST API ì œê³µ

RunPod ì„¤ì •:
1. Template: PyTorch 2.1 + CUDA 12.1
2. GPU: RTX 4090 (24GB) ì´ìƒ ê¶Œì¥
3. í¬íŠ¸: 8000 (HTTP)

í™˜ê²½ ë³€ìˆ˜:
- MODEL_NAME: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: microsoft/Phi-3-mini-4k-instruct)
- MAX_TOKENS: ìµœëŒ€ ìƒì„± í† í° (ê¸°ë³¸: 512)
- TEMPERATURE: ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0.7)

ì§€ì› ëª¨ë¸:
- microsoft/Phi-3-mini-4k-instruct (3.8B, ì¸ì¦ ë¶ˆí•„ìš”)
- meta-llama/Llama-3.2-3B-Instruct (ì¸ì¦ í•„ìš” - Hugging Face ë¡œê·¸ì¸ & ìŠ¹ì¸)

ì‹¤í–‰:
python runpod_llama_server.py
"""

import os
import logging
import re
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="RunPod LLaMA Model Server",
    description="íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ì„ ìœ„í•œ LLaMA ëª¨ë¸ ì„œë²„",
    version="1.0.0"
)

# CORS ì„¤ì • (Django ë°±ì—”ë“œì—ì„œ ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
device = None

# í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥
MODEL_NAME = os.getenv("MODEL_NAME", "microsoft/Phi-3-mini-4k-instruct")
MAX_MODEL_TOKENS = int(os.getenv("MAX_TOKENS", "512"))
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))


class GenerateRequest(BaseModel):
    """ìƒì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€", min_length=1)
    file_content: Optional[str] = Field(None, description="ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©")
    conversation_history: Optional[List[Dict]] = Field(
        None,
        description="ì´ì „ ëŒ€í™” ë‚´ì—­ [{'type': 'user'|'ai', 'content': '...'}]"
    )
    max_tokens: int = Field(
        MAX_MODEL_TOKENS,
        description="ìµœëŒ€ ìƒì„± í† í° ìˆ˜",
        ge=1,
        le=2048
    )
    temperature: float = Field(
        DEFAULT_TEMPERATURE,
        description="ìƒì„± ì˜¨ë„ (0.0~2.0)",
        ge=0.0,
        le=2.0
    )


class GenerateResponse(BaseModel):
    """ìƒì„± ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    response: str = Field(..., description="AI ì‘ë‹µ í…ìŠ¤íŠ¸")
    tokens_used: int = Field(..., description="ì‚¬ìš©ëœ í† í° ìˆ˜")
    model: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„")


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    status: str
    model_loaded: bool
    model_name: str
    device: str
    gpu_available: bool
    gpu_name: Optional[str] = None


def load_model():
    """ëª¨ë¸ ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰)"""
    global model, tokenizer, device

    logger.info("=" * 60)
    logger.info("RunPod LLaMA ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì‹œì‘")
    logger.info("=" * 60)

    # GPU í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    if device == "cuda":
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… GPU: {gpu_name}")
        logger.info(f"âœ… GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
    else:
        logger.warning("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    # ëª¨ë¸ ë¡œë”©
    logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}")

    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("1/3: í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )

        # pad_tokenì´ ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        logger.info("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ ë¡œë“œ
        logger.info("2/3: ëª¨ë¸ ë¡œë”©... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        if device == "cpu":
            model = model.to(device)

        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ í‰ê°€ ëª¨ë“œ
        logger.info("3/3: ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì„¤ì •...")
        model.eval()

        logger.info("=" * 60)
        logger.info("âœ… ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"   - ëª¨ë¸: {MODEL_NAME}")
        logger.info(f"   - ë””ë°”ì´ìŠ¤: {device}")
        logger.info(f"   - ìµœëŒ€ í† í°: {MAX_MODEL_TOKENS}")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        raise


def clean_non_korean_text(text: str) -> str:
    """
    í•œêµ­ì–´ê°€ ì•„ë‹Œ ë¬¸ìë¥¼ ì œê±°í•˜ê³  í•œêµ­ì–´ë§Œ ë‚¨ê¹€

    í—ˆìš©:
    - í•œê¸€ (ê°€-í£, ã„±-ã…, ã…-ã…£)
    - ì˜ë¬¸ (A-Z, a-z) - ê¸°ìˆ  ìš©ì–´ìš©
    - ìˆ«ì (0-9)
    - ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ (.,!?()[]{}'" ë“±)
    - ê³µë°±

    ì œê±°:
    - ë² íŠ¸ë‚¨ì–´, ì¤‘êµ­ì–´, ì¼ë³¸ì–´, íƒœêµ­ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ì˜ ë¬¸ì
    """
    # í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê¸°ë³¸ ë¬¸ì¥ë¶€í˜¸ë§Œ í—ˆìš©
    allowed_pattern = r'[ê°€-í£ã„±-ã…ã…-ã…£a-zA-Z0-9\s.,!?()[\]{}\'"Â·â€»â†’â†â†‘â†“âˆšÃ—Ã·Â±â‰¤â‰¥â‰ âˆâˆ‘âˆ«-]'

    # ë¬¸ì ë‹¨ìœ„ë¡œ í•„í„°ë§
    cleaned = ''.join(char for char in text if re.match(allowed_pattern, char))

    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()

    return cleaned


def extract_explicit_memory(conversation_history: Optional[List[Dict]]) -> Dict[str, any]:
    """
    ëª…ì‹œì  ë©”ëª¨ë¦¬ ì¶”ì¶œ: ì¤‘ìš” ì •ë³´ë¥¼ Key-Value í˜•íƒœë¡œ ì €ì¥

    ë©€í‹°í„´ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ í™•ì‹¤í•œ ê¸°ì–µ ê´€ë¦¬
    """
    memory = {
        'facts': [],  # ì‚¬ìš©ìê°€ ë§í•œ ì‚¬ì‹¤ë“¤
        'preferences': [],  # ì‚¬ìš©ìì˜ ì„ í˜¸/ê³„íš
        'topics': set(),  # ëŒ€í™” ì£¼ì œ
        'last_mentioned': {}  # ê° ì‚¬ì‹¤ì´ ëª‡ í„´ ì „ì— ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€
    }

    if not conversation_history:
        return memory

    total_turns = len(conversation_history)

    for idx, msg in enumerate(conversation_history):
        if msg.get('type') != 'user':
            continue

        content = msg.get('content', '')
        turns_ago = total_turns - idx

        # íŒ¨í„´ ê¸°ë°˜ ì‚¬ì‹¤ ì¶”ì¶œ (íŠ¹í—ˆ/ë…¼ë¬¸ ë„ë©”ì¸ íŠ¹í™”)
        # "ë‚˜ëŠ” Xë‹¤" / "ë‚˜ X í• ê±°ì•¼" / "X ê²€ìƒ‰í•˜ê³  ì‹¶ì–´"
        patterns = [
            # íŠ¹í—ˆ ê´€ë ¨ íŒ¨í„´
            ('íŠ¹í—ˆ ê²€ìƒ‰', 'patent_search'),
            ('íŠ¹í—ˆ ì°¾', 'patent_search'),
            ('íŠ¹í—ˆ ë¶„ì„', 'patent_analysis'),
            ('íŠ¹í—ˆë¥¼', 'patent_interest'),
            # ë…¼ë¬¸ ê´€ë ¨ íŒ¨í„´
            ('ë…¼ë¬¸ ê²€ìƒ‰', 'paper_search'),
            ('ë…¼ë¬¸ ì°¾', 'paper_search'),
            ('ë…¼ë¬¸ì„', 'paper_interest'),
            ('ì—°êµ¬ ì°¾', 'research_interest'),
            # ì¼ë°˜ ì„ í˜¸ë„
            ('ì¢‹ì•„í•´', 'preference'),
            ('ì‹«ì–´í•´', 'dislike'),
            ('ê´€ì‹¬ìˆ', 'interest'),
        ]

        for pattern, fact_type in patterns:
            if pattern in content:
                # í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œ (íŒ¨í„´ ì•ì˜ ëª…ì‚¬)
                words = content.split()
                for i, word in enumerate(words):
                    if pattern in word and i > 0:
                        key_info = ' '.join(words[max(0, i-3):i])
                        memory['facts'].append({
                            'type': fact_type,
                            'content': key_info + ' ' + pattern,
                            'turns_ago': turns_ago
                        })
                        memory['last_mentioned'][fact_type] = turns_ago

        # ì£¼ì œ ì¶”ì¶œ
        if 'íŠ¹í—ˆ' in content or 'ê²Œì„' in content:
            memory['topics'].add('íŠ¹í—ˆ')
        if 'ë…¼ë¬¸' in content or 'ì—°êµ¬' in content:
            memory['topics'].add('ë…¼ë¬¸')

    return memory


def build_structured_summary(memory: Dict, conversation_history: Optional[List[Dict]]) -> str:
    """
    êµ¬ì¡°í™”ëœ ëŒ€í™” ìš”ì•½ ìƒì„±

    ëª…í™•í•œ í˜•ì‹ìœ¼ë¡œ ì¤‘ìš” ì •ë³´ë¥¼ ì •ë¦¬í•˜ì—¬ ëª¨ë¸ì´ í™•ì‹¤í•˜ê²Œ ì´í•´í•˜ë„ë¡ í•¨
    """
    if not memory['facts'] and not memory['topics']:
        return ""

    summary_parts = ["[ëŒ€í™” ë©”ëª¨ë¦¬]"]

    # ì£¼ì œ
    if memory['topics']:
        summary_parts.append(f"ì£¼ì œ: {', '.join(memory['topics'])}")

    # ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ì‚¬ì‹¤ë“¤ (ìµœê·¼ 5ê°œ)
    if memory['facts']:
        summary_parts.append("\ní•µì‹¬ ì‚¬ì‹¤:")
        for fact in memory['facts'][-5:]:
            summary_parts.append(
                f"  â€¢ {fact['content']} ({fact['turns_ago']}í„´ ì „)"
            )

    # ìµœê·¼ ê´€ë ¨ ëŒ€í™” (ì»¨í…ìŠ¤íŠ¸ìš©)
    if conversation_history and len(conversation_history) >= 2:
        recent = conversation_history[-2:]
        summary_parts.append("\nìµœê·¼ ëŒ€í™”:")
        for msg in recent:
            role = "ì‚¬ìš©ì" if msg['type'] == 'user' else "AI"
            content = msg['content'][:50] + "..." if len(msg['content']) > 50 else msg['content']
            summary_parts.append(f"  {role}: {content}")

    return "\n".join(summary_parts) + "\n\n"


def build_llama_prompt(
    message: str,
    file_content: Optional[str] = None,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    10í„´ ë‚´ í™•ì‹¤í•œ ê¸°ì–µì„ ìœ„í•œ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±

    ì „ëµ:
    1. ëª…ì‹œì  ë©”ëª¨ë¦¬: ì¤‘ìš” ì‚¬ì‹¤ì„ Key-Valueë¡œ ì¶”ì¶œ
    2. êµ¬ì¡°í™”ëœ ìš”ì•½: ëª…í™•í•œ í˜•ì‹ìœ¼ë¡œ ì •ë³´ ì •ë¦¬
    3. Few-Shot ì˜ˆì‹œ: ì˜¬ë°”ë¥¸ ê¸°ì–µ ë°©ë²• ì‹œë²”

    Args:
        message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        file_content: ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©
        conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­

    Returns:
        LLaMA í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
    """
    prompt_parts = ["<|begin_of_text|>"]

    # Few-Shot ì˜ˆì‹œê°€ í¬í•¨ëœ ì‹œìŠ¤í…œ ë©”ì‹œì§€ (íŠ¹í—ˆ ë„ë©”ì¸ íŠ¹í™” + í”„ë¡œì•¡í‹°ë¸Œ ê¸°ëŠ¥)
    system_message = (
        "ë‹¹ì‹ ì€ íŠ¹í—ˆ ë° ë…¼ë¬¸ ê²€ìƒ‰Â·ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n"
        "**IMPORTANT: You MUST respond ONLY in Korean language. Never use English, Vietnamese, Chinese, or any other language.**\n"
        "**ì¤‘ìš”: ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”. ì˜ì–´, ë² íŠ¸ë‚¨ì–´, ì¤‘êµ­ì–´ ë“± ë‹¤ë¥¸ ì–¸ì–´ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**\n\n"
        "**ì¤‘ìš”: ëŒ€í™” ë‚´ìš©ì„ ì •í™•íˆ ê¸°ì–µí•˜ëŠ” ë°©ë²•**\n\n"
        "ì˜ˆì‹œ 1 - íŠ¹í—ˆ ê²€ìƒ‰ ê¸°ì–µ:\n"
        "ì‚¬ìš©ì: ê²Œì„ íŠ¹í—ˆ ê²€ìƒ‰í•˜ê³  ì‹¶ì–´\n"
        "AI: ê²Œì„ ê´€ë ¨ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì–´ë–¤ ì¢…ë¥˜ì˜ ê²Œì„ì¸ê°€ìš”?\n"
        "ì‚¬ìš©ì: ë‚´ê°€ ë¬´ìŠ¨ íŠ¹í—ˆ ì°¾ëŠ”ë‹¤ê³  í–ˆì§€?\n"
        "âœ“ ì •ë‹µ: 'ê²Œì„ íŠ¹í—ˆë¥¼ ê²€ìƒ‰í•˜ì‹ ë‹¤ê³  í•˜ì…¨ìŠµë‹ˆë‹¤ (2í„´ ì „)'\n"
        "âœ— ì˜¤ë‹µ: 'ê¸°ì–µì´ ì•ˆ ë‚˜ìš”' / 'ë­”ê°€ íŠ¹í—ˆë¥¼...'\n\n"
        "ì˜ˆì‹œ 2 - ë…¼ë¬¸ ì£¼ì œ ê¸°ì–µ:\n"
        "ì‚¬ìš©ì: ë”¥ëŸ¬ë‹ ë…¼ë¬¸ ì°¾ì•„ë³´ëŠ” ì¤‘ì´ì•¼\n"
        "AI: ë”¥ëŸ¬ë‹ ë¶„ì•¼ ë…¼ë¬¸ì„ ì°¾ê³  ê³„ì‹œëŠ”êµ°ìš”. ì–´ë–¤ ì„¸ë¶€ ì£¼ì œì— ê´€ì‹¬ì´ ìˆìœ¼ì‹ ê°€ìš”?\n"
        "ì‚¬ìš©ì: ë‚´ê°€ ì°¾ëŠ” ë…¼ë¬¸ ì£¼ì œê°€ ë­ì˜€ì§€?\n"
        "âœ“ ì •ë‹µ: 'ë”¥ëŸ¬ë‹ ë…¼ë¬¸ì„ ì°¾ê³  ê³„ì…¨ìŠµë‹ˆë‹¤ (2í„´ ì „)'\n\n"
        "ì˜ˆì‹œ 3 - í”„ë¡œì•¡í‹°ë¸Œ íŠ¹í—ˆ ë¶„ì„:\n"
        "ì‚¬ìš©ì: ê³¨í”„ ê²Œì„ íŠ¹í—ˆ ê²€ìƒ‰ ì¤‘ì´ì•¼\n"
        "AI: ê³¨í”„ ê²Œì„ íŠ¹í—ˆë¥¼ ì°¾ê³  ê³„ì‹œëŠ”êµ°ìš”! ê²€ìƒ‰í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
        "ì‚¬ìš©ì: ì¢‹ì€ ê²€ìƒ‰ ë°©ë²• ìˆì–´?\n"
        "âœ“ ì •ë‹µ: 'ê³¨í”„ ê²Œì„ íŠ¹í—ˆëŠ” IPC ë¶„ë¥˜ A63Bë‚˜ G06Të¥¼ í™œìš©í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œë¡œëŠ” \"golf simulation\", \"swing analysis\" ë“±ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.'\n"
        "âœ— ì˜¤ë‹µ: 'ì¼ë°˜ì ìœ¼ë¡œ í‚¤ì›Œë“œ ê²€ìƒ‰ì´ ì¢‹ìŠµë‹ˆë‹¤' (ê³¨í”„ ê²Œì„ ë§¥ë½ ë¬´ì‹œ)\n\n"
        "ì˜ˆì‹œ 4 - í”„ë¡œì•¡í‹°ë¸Œ ìœ ì‚¬ íŠ¹í—ˆ:\n"
        "ì‚¬ìš©ì: ë³´ë“œê²Œì„ íŠ¹í—ˆ ë¶„ì„í•´ì¤˜\n"
        "AI: ë³´ë“œê²Œì„ íŠ¹í—ˆ ë¶„ì„í•´ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
        "ì‚¬ìš©ì: ìœ ì‚¬í•œ íŠ¹í—ˆë„ ìˆì–´?\n"
        "âœ“ ì •ë‹µ: 'ë„¤! ë³´ë“œê²Œì„ê³¼ ìœ ì‚¬í•œ íŠ¹í—ˆë¡œëŠ” ì¹´ë“œê²Œì„, í¼ì¦ê²Œì„ ê´€ë ¨ íŠ¹í—ˆë“¤ì´ ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰í•´ë“œë¦´ê¹Œìš”?'\n"
        "âœ— ì˜¤ë‹µ: 'ìœ ì‚¬ íŠ¹í—ˆê°€ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤' (ë³´ë“œê²Œì„ ë§¥ë½ ë¬´ì‹œ)\n\n"
        "ì˜ˆì‹œ 5 - í”„ë¡œì•¡í‹°ë¸Œ ë…¼ë¬¸ ì—°ê³„:\n"
        "ì‚¬ìš©ì: ìì—°ì–´ì²˜ë¦¬ ë…¼ë¬¸ ê²€ìƒ‰í•´ì¤˜\n"
        "AI: ìì—°ì–´ì²˜ë¦¬ ë…¼ë¬¸ì„ ì°¾ì•„ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n"
        "ì‚¬ìš©ì: ìµœì‹  ì—°êµ¬ ë™í–¥ë„ ì•Œë ¤ì¤˜\n"
        "âœ“ ì •ë‹µ: 'ìì—°ì–´ì²˜ë¦¬ ë¶„ì•¼ì—ì„œëŠ” ìµœê·¼ LLM(ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸), RAG, í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ì—°êµ¬ê°€ í™œë°œí•©ë‹ˆë‹¤. ê´€ë ¨ ë…¼ë¬¸ì„ ê²€ìƒ‰í•´ë“œë¦´ê¹Œìš”?'\n"
        "âœ— ì˜¤ë‹µ: 'AI ì—°êµ¬ê°€ í™œë°œí•©ë‹ˆë‹¤' (ìì—°ì–´ì²˜ë¦¬ ë§¥ë½ ë¬´ì‹œ)\n\n"
        "**ì‘ë‹µ ê·œì¹™:**\n"
        "1. [ëŒ€í™” ë©”ëª¨ë¦¬] ì„¹ì…˜ì˜ ì •ë³´ë¥¼ ë°˜ë“œì‹œ ì°¸ì¡°í•˜ì„¸ìš”\n"
        "2. íŠ¹í—ˆ/ë…¼ë¬¸ ê´€ë ¨ ì´ì „ ì–¸ê¸‰ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•˜ì„¸ìš” (ë¬¼ì–´ë³´ì§€ ì•Šì•„ë„!)\n"
        "3. ëª‡ í„´ ì „ì— ì–¸ê¸‰ë˜ì—ˆëŠ”ì§€ ì•Œë ¤ì£¼ë©´ ë” ì¢‹ìŠµë‹ˆë‹¤\n"
        "4. ì •í™•í•œ ì •ë³´ë§Œ ë‹µë³€í•˜ê³ , ë¶ˆí™•ì‹¤í•˜ë©´ ì†”ì§íˆ ë§í•˜ì„¸ìš”\n"
        "5. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•˜ì„¸ìš”"
    )

    prompt_parts.append("<|start_header_id|>system<|end_header_id|>")
    prompt_parts.append(f"{system_message}<|eot_id|>")

    # ëª…ì‹œì  ë©”ëª¨ë¦¬ ì¶”ì¶œ + êµ¬ì¡°í™”ëœ ìš”ì•½ ìƒì„±
    memory = extract_explicit_memory(conversation_history)
    structured_summary = build_structured_summary(memory, conversation_history)

    # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€ (ë©”ëª¨ë¦¬ê°€ ìˆìœ¼ë©´ ìµœì†Œí™”)
    max_history = 4 if structured_summary else 8

    if conversation_history:
        for hist in conversation_history[-max_history:]:
            role = "user" if hist['type'] == 'user' else "assistant"
            prompt_parts.append(f"<|start_header_id|>{role}<|end_header_id|>")
            prompt_parts.append(f"{hist['content']}<|eot_id|>")

    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
    prompt_parts.append("<|start_header_id|>user<|end_header_id|>")

    # ë©”ì‹œì§€ êµ¬ì„±
    user_message_parts = []

    # êµ¬ì¡°í™”ëœ ë©”ëª¨ë¦¬ (ê°€ì¥ ì¤‘ìš”!)
    if structured_summary:
        user_message_parts.append(structured_summary)

    # íŒŒì¼ ë‚´ìš©
    if file_content:
        user_message_parts.append(f"[ì²¨ë¶€ íŒŒì¼]\n{file_content[:1000]}\n")

    # ì‹¤ì œ ì§ˆë¬¸
    user_message_parts.append(f"[ì§ˆë¬¸]\n{message}")

    prompt_parts.append("\n".join(user_message_parts) + "<|eot_id|>")

    # AI ì‘ë‹µ ì‹œì‘
    prompt_parts.append("<|start_header_id|>assistant<|end_header_id|>")

    return "\n".join(prompt_parts)


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()


@app.get("/", response_model=HealthResponse)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - í—¬ìŠ¤ì²´í¬"""
    gpu_name = None
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)

    return HealthResponse(
        status="ok",
        model_loaded=model is not None,
        model_name=MODEL_NAME,
        device=str(device),
        gpu_available=torch.cuda.is_available(),
        gpu_name=gpu_name
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return await root()


@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """
    ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ëŠ” í…ìŠ¤íŠ¸ ìƒì„±

    íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ ì§ˆë¬¸ì— ëŒ€í•´ LLaMA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if model is None or tokenizer is None:
        raise HTTPException(
            status_code=503,
            detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )

    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_llama_prompt(
            message=request.message,
            file_content=request.file_content,
            conversation_history=request.conversation_history
        )

        logger.info(f"ğŸ“ ìƒì„± ìš”ì²­: {request.message[:100]}...")
        logger.info(f"   - max_tokens: {request.max_tokens}")
        logger.info(f"   - temperature: {request.temperature}")
        logger.info(f"   - ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(request.conversation_history) if request.conversation_history else 0}ê°œ")

        # í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048  # ì…ë ¥ ê¸¸ì´ ì œí•œ
        ).to(device)

        # ì‘ë‹µ ìƒì„± (í•œêµ­ì–´ ê°•ì œ ìœ ì§€)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                min_new_tokens=20,  # ìµœì†Œ 20í† í° (50â†’20ìœ¼ë¡œ ê°ì†Œ)
                temperature=request.temperature,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.5,  # 1.2â†’1.5 (ë°˜ë³µ ì–µì œ ê°•í™”)
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3,
                use_cache=False,
                # í•œêµ­ì–´ ê°•ì œ ì„¤ì •
                forced_bos_token_id=None,  # ì‹œì‘ í† í° ììœ ë¡­ê²Œ
                exponential_decay_length_penalty=(1, 1.05)  # ì§§ì€ ë‹µë³€ ì„ í˜¸
            )

        # ë””ì½”ë”©
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)

        # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        # <|start_header_id|>assistant<|end_header_id|> ì´í›„ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ
        response_start = full_response.rfind("<|start_header_id|>assistant<|end_header_id|>")
        if response_start != -1:
            response_text = full_response[response_start + len("<|start_header_id|>assistant<|end_header_id|>"):]
            # <|eot_id|> ì œê±°
            response_text = response_text.replace("<|eot_id|>", "").strip()
            response_text = response_text.replace("<|end_of_text|>", "").strip()
        else:
            response_text = full_response

        # í•œêµ­ì–´ê°€ ì•„ë‹Œ ì–¸ì–´ ì œê±° (ë² íŠ¸ë‚¨ì–´, ì¤‘êµ­ì–´ ë“±)
        original_length = len(response_text)
        response_text = clean_non_korean_text(response_text)
        if len(response_text) < original_length:
            logger.warning(f"âš ï¸ ë¹„í•œêµ­ì–´ ë¬¸ì ì œê±°: {original_length} â†’ {len(response_text)} ê¸€ì")

        # í† í° ìˆ˜ ê³„ì‚°
        tokens_used = outputs[0].shape[0] - inputs['input_ids'].shape[1]

        logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {len(response_text)} ê¸€ì, {tokens_used} í† í°")

        return GenerateResponse(
            response=response_text,
            tokens_used=tokens_used,
            model=MODEL_NAME
        )

    except torch.cuda.OutOfMemoryError:
        logger.error("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
        raise HTTPException(
            status_code=507,
            detail="GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. max_tokensë¥¼ ì¤„ì´ê±°ë‚˜ ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.get("/model-info")
async def model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if model is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    info = {
        "model_name": MODEL_NAME,
        "device": str(device),
        "max_tokens": MAX_MODEL_TOKENS,
        "default_temperature": DEFAULT_TEMPERATURE,
    }

    if torch.cuda.is_available():
        info["gpu_name"] = torch.cuda.get_device_name(0)
        info["gpu_memory_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.1f} GB"
        info["gpu_memory_reserved"] = f"{torch.cuda.memory_reserved(0) / 1024**3:.1f} GB"

    return info


if __name__ == "__main__":
    # ì„œë²„ ì‹¤í–‰
    port = int(os.getenv("PORT", "8000"))
    host = os.getenv("HOST", "0.0.0.0")

    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘: http://{host}:{port}")
    logger.info(f"ğŸ“– API ë¬¸ì„œ: http://{host}:{port}/docs")

    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True
    )
