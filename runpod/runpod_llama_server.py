"""
RunPodìš© LLaMA ëª¨ë¸ ì„œë²„
GPU ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì‹¤í–‰ë˜ë©° FastAPIë¡œ REST API ì œê³µ

RunPod ì„¤ì •:
1. Template: PyTorch 2.1 + CUDA 12.1
2. GPU: RTX 4090 (24GB) ì´ìƒ ê¶Œì¥
3. í¬íŠ¸: 8000 (HTTP)

í™˜ê²½ ë³€ìˆ˜:
- MODEL_NAME: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸: microsoft/Phi-3-mini-4k-instruct)
- MAX_TOKENS: ìµœëŒ€ ìƒì„± í† í° (ê¸°ë³¸: 512)
- TEMPERATURE: ìƒì„± ì˜¨ë„ (ê¸°ë³¸: 0.7)

ì§€ì› ëª¨ë¸:
- microsoft/Phi-3-mini-4k-instruct (3.8B, ì¸ì¦ ë¶ˆí•„ìš”)
- meta-llama/Llama-3.2-3B-Instruct (ì¸ì¦ í•„ìš” - Hugging Face ë¡œê·¸ì¸ & ìŠ¹ì¸)

ì‹¤í–‰:
python runpod_llama_server.py
"""

import os
import logging
from typing import List, Dict, Optional
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import uvicorn

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI ì•±
app = FastAPI(
    title="RunPod LLaMA Model Server",
    description="íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ì„ ìœ„í•œ LLaMA ëª¨ë¸ ì„œë²„",
    version="1.0.0"
)

# CORS ì„¤ì • (Django ë°±ì—”ë“œì—ì„œ ì ‘ê·¼ í—ˆìš©)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # í”„ë¡œë•ì…˜ì—ì„œëŠ” íŠ¹ì • ë„ë©”ì¸ë§Œ í—ˆìš©
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì „ì—­ ë³€ìˆ˜
model = None
tokenizer = None
device = None

# í™˜ê²½ ë³€ìˆ˜ë¡œ ì„¤ì • ê°€ëŠ¥
MODEL_NAME = os.getenv("MODEL_NAME", "microsoft/Phi-3-mini-4k-instruct")
MAX_MODEL_TOKENS = int(os.getenv("MAX_TOKENS", "512"))
DEFAULT_TEMPERATURE = float(os.getenv("TEMPERATURE", "0.7"))


class GenerateRequest(BaseModel):
    """ìƒì„± ìš”ì²­ ìŠ¤í‚¤ë§ˆ"""
    message: str = Field(..., description="ì‚¬ìš©ì ë©”ì‹œì§€", min_length=1)
    file_content: Optional[str] = Field(None, description="ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©")
    conversation_history: Optional[List[Dict]] = Field(
        None,
        description="ì´ì „ ëŒ€í™” ë‚´ì—­ [{'type': 'user'|'ai', 'content': '...'}]"
    )
    max_tokens: int = Field(
        MAX_MODEL_TOKENS,
        description="ìµœëŒ€ ìƒì„± í† í° ìˆ˜",
        ge=1,
        le=2048
    )
    temperature: float = Field(
        DEFAULT_TEMPERATURE,
        description="ìƒì„± ì˜¨ë„ (0.0~2.0)",
        ge=0.0,
        le=2.0
    )


class GenerateResponse(BaseModel):
    """ìƒì„± ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    response: str = Field(..., description="AI ì‘ë‹µ í…ìŠ¤íŠ¸")
    tokens_used: int = Field(..., description="ì‚¬ìš©ëœ í† í° ìˆ˜")
    model: str = Field(..., description="ì‚¬ìš©ëœ ëª¨ë¸ ì´ë¦„")


class HealthResponse(BaseModel):
    """í—¬ìŠ¤ì²´í¬ ì‘ë‹µ ìŠ¤í‚¤ë§ˆ"""
    status: str
    model_loaded: bool
    model_name: str
    device: str
    gpu_available: bool
    gpu_name: Optional[str] = None


def load_model():
    """ëª¨ë¸ ì´ˆê¸°í™” (ì„œë²„ ì‹œì‘ ì‹œ 1íšŒ ì‹¤í–‰)"""
    global model, tokenizer, device

    logger.info("=" * 60)
    logger.info("RunPod LLaMA ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì‹œì‘")
    logger.info("=" * 60)

    # GPU í™•ì¸
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"âœ… ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")

    if device == "cuda":
        gpu_name = torch.cuda.get_device_name(0)
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
        logger.info(f"âœ… GPU: {gpu_name}")
        logger.info(f"âœ… GPU ë©”ëª¨ë¦¬: {gpu_memory:.1f} GB")
    else:
        logger.warning("âš ï¸  GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    # ëª¨ë¸ ë¡œë”©
    logger.info(f"ğŸ“¥ ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}")

    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        logger.info("1/3: í† í¬ë‚˜ì´ì € ë¡œë”©...")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            trust_remote_code=True
        )

        # pad_tokenì´ ì—†ìœ¼ë©´ eos_tokenìœ¼ë¡œ ì„¤ì •
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        logger.info("âœ… í† í¬ë‚˜ì´ì € ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ ë¡œë“œ
        logger.info("2/3: ëª¨ë¸ ë¡œë”©... (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        if device == "cpu":
            model = model.to(device)

        logger.info("âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ")

        # ëª¨ë¸ í‰ê°€ ëª¨ë“œ
        logger.info("3/3: ëª¨ë¸ í‰ê°€ ëª¨ë“œ ì„¤ì •...")
        model.eval()

        logger.info("=" * 60)
        logger.info("âœ… ëª¨ë¸ ì„œë²„ ì´ˆê¸°í™” ì™„ë£Œ!")
        logger.info(f"   - ëª¨ë¸: {MODEL_NAME}")
        logger.info(f"   - ë””ë°”ì´ìŠ¤: {device}")
        logger.info(f"   - ìµœëŒ€ í† í°: {MAX_MODEL_TOKENS}")
        logger.info("=" * 60)

    except Exception as e:
        logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        raise


def extract_conversation_context(conversation_history: Optional[List[Dict]]) -> str:
    """
    ëŒ€í™” íˆìŠ¤í† ë¦¬ì—ì„œ í•µì‹¬ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ (RAG ê°œë… ì ìš©)

    ë©€í‹°í„´ ì„±ëŠ¥ í–¥ìƒ ì „ëµ:
    - ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ì£¼ìš” ì—”í‹°í‹°(íŠ¹í—ˆëª…, í‚¤ì›Œë“œ ë“±) ì¶”ì¶œ
    - ëŒ€í™”ì˜ ì£¼ì œì™€ ë§¥ë½ íŒŒì•…
    - ìµœê·¼ 3í„´ì˜ ìš”ì•½ ìƒì„±
    """
    if not conversation_history or len(conversation_history) < 2:
        return ""

    # ìµœê·¼ 6ê°œ ë©”ì‹œì§€ ë¶„ì„ (3í„´)
    recent_messages = conversation_history[-6:]

    # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)
    entities = []
    topics = []

    for msg in recent_messages:
        content = msg.get('content', '')

        # íŠ¹í—ˆ/ë…¼ë¬¸ ê´€ë ¨ í‚¤ì›Œë“œ
        if 'íŠ¹í—ˆ' in content or 'ê²Œì„' in content or 'ê³¨í”„' in content or 'ë³´ë“œ' in content:
            topics.append('íŠ¹í—ˆ')
        if 'ë…¼ë¬¸' in content or 'ì—°êµ¬' in content:
            topics.append('ë…¼ë¬¸')

        # êµ¬ì²´ì  ì—”í‹°í‹° (ì˜ˆ: "ì¹˜í‚¨", "ê²Œì„ íŠ¹í—ˆ" ë“±)
        words = content.split()
        for word in words:
            if len(word) >= 2 and word not in ['ì´ê±°', 'ì €ê±°', 'ê·¸ê±°', 'ë­ì•¼', 'ì–´ë–»ê²Œ']:
                entities.append(word)

    # ì¤‘ë³µ ì œê±° ë° ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
    unique_topics = list(set(topics))
    unique_entities = list(set(entities))[-5:]  # ìµœê·¼ 5ê°œë§Œ

    context_parts = []
    if unique_topics:
        context_parts.append(f"ëŒ€í™” ì£¼ì œ: {', '.join(unique_topics)}")
    if unique_entities:
        context_parts.append(f"ì–¸ê¸‰ëœ ë‚´ìš©: {', '.join(unique_entities)}")

    if context_parts:
        return "[ëŒ€í™” ì»¨í…ìŠ¤íŠ¸]\n" + "\n".join(context_parts) + "\n\n"
    return ""


def build_llama_prompt(
    message: str,
    file_content: Optional[str] = None,
    conversation_history: Optional[List[Dict]] = None
) -> str:
    """
    ë©€í‹°í„´ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ LLaMA 3.2 í”„ë¡¬í”„íŠ¸ ìƒì„±

    ê°œì„  ì‚¬í•­:
    1. RAG ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬: ëŒ€í™”ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    2. ëª…í™•í•œ ì§€ì¹¨: ì—­í• ê³¼ ì‘ë‹µ ìŠ¤íƒ€ì¼ ëª…ì‹œ
    3. ëŒ€í™” ìƒíƒœ ê´€ë¦¬: ì´ì „ ë§¥ë½ ìš”ì•½ ì œê³µ

    Args:
        message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
        file_content: ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš©
        conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­

    Returns:
        LLaMA í˜•ì‹ì˜ í”„ë¡¬í”„íŠ¸
    """
    prompt_parts = ["<|begin_of_text|>"]

    # ê°œì„ ëœ ì‹œìŠ¤í…œ ë©”ì‹œì§€ (ëª…í™•í•œ ì§€ì¹¨)
    system_message = (
        "ë‹¹ì‹ ì€ íŠ¹í—ˆ ë° ë…¼ë¬¸ ê²€ìƒ‰Â·ë¶„ì„ ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\n\n"
        "ì—­í• :\n"
        "- íŠ¹í—ˆ ë¬¸ì„œ, ë…¼ë¬¸, ê¸°ìˆ  ë¶„ì„ì— ëŒ€í•œ ì „ë¬¸ì ì´ê³  ìƒì„¸í•œ ë‹µë³€ ì œê³µ\n"
        "- ì´ì „ ëŒ€í™” ë§¥ë½ì„ ê¸°ì–µí•˜ê³  ì—°ì†ì„± ìˆëŠ” ëŒ€í™” ì§„í–‰\n"
        "- ì‚¬ìš©ìê°€ ì–¸ê¸‰í•œ ë‚´ìš©ì„ ì°¸ì¡°í•˜ì—¬ ë‹µë³€\n\n"
        "ì‘ë‹µ ê·œì¹™:\n"
        "1. ì´ì „ ëŒ€í™”ì—ì„œ ì–¸ê¸‰ëœ ë‚´ìš©ì€ ë°˜ë“œì‹œ ê¸°ì–µí•˜ê³  ì°¸ì¡°í•  ê²ƒ\n"
        "2. ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µë³€í•  ê²ƒ\n"
        "3. ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰½ê²Œ ì„¤ëª…í•  ê²ƒ\n"
        "4. ë¶ˆí™•ì‹¤í•œ ê²½ìš° ì¶”ì¸¡í•˜ì§€ ë§ê³  ì •ì§í•˜ê²Œ ë‹µë³€í•  ê²ƒ\n"
        "5. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ë‹µí•  ê²ƒ"
    )

    prompt_parts.append("<|start_header_id|>system<|end_header_id|>")
    prompt_parts.append(f"{system_message}<|eot_id|>")

    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ìš”ì•½
    conversation_context = extract_conversation_context(conversation_history)

    # ì´ì „ ëŒ€í™” ë‚´ì—­ ì¶”ê°€ (ìµœê·¼ 8ê°œ = 4í„´)
    # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ë” ì ì€ íˆìŠ¤í† ë¦¬ë¡œë„ ì¶©ë¶„
    max_history = 6 if conversation_context else 10

    if conversation_history:
        for hist in conversation_history[-max_history:]:
            role = "user" if hist['type'] == 'user' else "assistant"
            prompt_parts.append(f"<|start_header_id|>{role}<|end_header_id|>")
            prompt_parts.append(f"{hist['content']}<|eot_id|>")

    # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
    prompt_parts.append("<|start_header_id|>user<|end_header_id|>")

    # ì»¨í…ìŠ¤íŠ¸ + ë©”ì‹œì§€ êµ¬ì„±
    user_message_parts = []

    # ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ (RAG)
    if conversation_context:
        user_message_parts.append(conversation_context)

    # íŒŒì¼ ë‚´ìš©
    if file_content:
        user_message_parts.append(f"[ì²¨ë¶€ íŒŒì¼]\n{file_content[:1000]}\n")

    # ì‹¤ì œ ì§ˆë¬¸
    user_message_parts.append(f"[ì§ˆë¬¸]\n{message}")

    prompt_parts.append("\n".join(user_message_parts) + "<|eot_id|>")

    # AI ì‘ë‹µ ì‹œì‘
    prompt_parts.append("<|start_header_id|>assistant<|end_header_id|>")

    return "\n".join(prompt_parts)


@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ë¡œë“œ"""
    load_model()


@app.get("/", response_model=HealthResponse)
async def root():
    """ë£¨íŠ¸ ì—”ë“œí¬ì¸íŠ¸ - í—¬ìŠ¤ì²´í¬"""
    gpu_name = None
    if torch.cuda.is_available():
        gpu_name = torch.cuda.get_device_name(0)

    return HealthResponse(
        status="ok",
        model_loaded=model is not None,
        model_name=MODEL_NAME,
        device=str(device),
        gpu_available=torch.cuda.is_available(),
        gpu_name=gpu_name
    )


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return await root()


@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """
    ë©€í‹°í„´ ëŒ€í™”ë¥¼ ì§€ì›í•˜ëŠ” í…ìŠ¤íŠ¸ ìƒì„±

    íŠ¹í—ˆ ê²€ìƒ‰ ë° ë¶„ì„ ì§ˆë¬¸ì— ëŒ€í•´ LLaMA ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    if model is None or tokenizer is None:
        raise HTTPException(
            status_code=503,
            detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )

    try:
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = build_llama_prompt(
            message=request.message,
            file_content=request.file_content,
            conversation_history=request.conversation_history
        )

        logger.info(f"ğŸ“ ìƒì„± ìš”ì²­: {request.message[:100]}...")
        logger.info(f"   - max_tokens: {request.max_tokens}")
        logger.info(f"   - temperature: {request.temperature}")
        logger.info(f"   - ëŒ€í™” íˆìŠ¤í† ë¦¬: {len(request.conversation_history) if request.conversation_history else 0}ê°œ")

        # í† í°í™”
        inputs = tokenizer(
            prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048  # ì…ë ¥ ê¸¸ì´ ì œí•œ
        ).to(device)

        # ì‘ë‹µ ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                min_new_tokens=50,  # ìµœì†Œ 50í† í° ìƒì„±
                temperature=request.temperature,
                do_sample=True,
                top_p=0.9,
                top_k=50,
                repetition_penalty=1.2,  # 1.1 â†’ 1.2 (ë°˜ë³µ ê°ì†Œ)
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                no_repeat_ngram_size=3  # 3-gram ë°˜ë³µ ë°©ì§€
            )

        # ë””ì½”ë”©
        full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)

        # ì‘ë‹µ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        # <|start_header_id|>assistant<|end_header_id|> ì´í›„ ë§ˆì§€ë§‰ ë¶€ë¶„ë§Œ
        response_start = full_response.rfind("<|start_header_id|>assistant<|end_header_id|>")
        if response_start != -1:
            response_text = full_response[response_start + len("<|start_header_id|>assistant<|end_header_id|>"):]
            # <|eot_id|> ì œê±°
            response_text = response_text.replace("<|eot_id|>", "").strip()
            response_text = response_text.replace("<|end_of_text|>", "").strip()
        else:
            response_text = full_response

        # í† í° ìˆ˜ ê³„ì‚°
        tokens_used = outputs[0].shape[0] - inputs['input_ids'].shape[1]

        logger.info(f"âœ… ìƒì„± ì™„ë£Œ: {len(response_text)} ê¸€ì, {tokens_used} í† í°")

        return GenerateResponse(
            response=response_text,
            tokens_used=tokens_used,
            model=MODEL_NAME
        )

    except torch.cuda.OutOfMemoryError:
        logger.error("âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±")
        raise HTTPException(
            status_code=507,
            detail="GPU ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. max_tokensë¥¼ ì¤„ì´ê±°ë‚˜ ì„œë²„ë¥¼ ì¬ì‹œì‘í•´ì£¼ì„¸ìš”."
        )
    except Exception as e:
        logger.error(f"âŒ ì‘ë‹µ ìƒì„± ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
        )


@app.get("/model-info")
async def model_info():
    """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
    if model is None:
        raise HTTPException(status_code=503, detail="ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

    info = {
        "model_name": MODEL_NAME,
        "device": str(device),
        "max_tokens": MAX_MODEL_TOKENS,
        "default_temperature": DEFAULT_TEMPERATURE,
    }

    if torch.cuda.is_available():
        info["gpu_name"] = torch.cuda.get_device_name(0)
        info["gpu_memory_total"] = f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB"
        info["gpu_memory_allocated"] = f"{torch.cuda.memory_allocated(0) / 1024**3:.1f} GB"
        info["gpu_memory_reserved"] = f"{torch.cuda.memory_reserved(0) / 1024**3:.1f} GB"

    return info


if __name__ == "__main__":
    # ì„œë²„ ì‹¤í–‰
    port = int(os.getenv("PORT", "8000"))
    host = os.getenv("HOST", "0.0.0.0")

    logger.info(f"ğŸš€ ì„œë²„ ì‹œì‘: http://{host}:{port}")
    logger.info(f"ğŸ“– API ë¬¸ì„œ: http://{host}:{port}/docs")

    uvicorn.run(
        app,
        host=host,
        port=port,
        log_level="info",
        access_log=True
    )
