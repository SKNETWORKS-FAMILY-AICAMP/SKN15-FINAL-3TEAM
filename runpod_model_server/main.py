"""
Runpod ëª¨ë¸ ì„œë²„ - FastAPI
BGE-M3 ì„ë² ë”©, Qwen2.5 ë¶„ë¥˜ ëª¨ë¸, Qwen2.5-14B ì±—ë´‡
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM
from peft import PeftModel
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Patent RAG Model Server with Classification and LLM")

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"ğŸ’» Device: {device}")

# ëª¨ë¸ ë¡œë”© (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
logger.info("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...")

# 1. BGE-M3 ì„ë² ë”© ëª¨ë¸
logger.info("ğŸ“¦ BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
embedding_model = SentenceTransformer('BAAI/bge-m3', device=device)
logger.info("âœ… BGE-M3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# 2. Qwen2.5-7B ë¶„ë¥˜ ëª¨ë¸ (LoRA ì–´ëŒ‘í„°)
logger.info("ğŸ“¦ Qwen2.5-7B ë¶„ë¥˜ ëª¨ë¸ ë¡œë”©...")
classification_base_model_name = "Qwen/Qwen2.5-7B-Instruct"
classification_adapter_path = "/workspace/models/classification"  # Runpodì—ì„œ ëª¨ë¸ ê²½ë¡œ

try:
    # ë¶„ë¥˜ ëª¨ë¸ í† í¬ë‚˜ì´ì €
    classification_tokenizer = AutoTokenizer.from_pretrained(
        classification_base_model_name,
        trust_remote_code=True
    )

    # ë¶„ë¥˜ ë² ì´ìŠ¤ ëª¨ë¸
    classification_base_model = AutoModelForSequenceClassification.from_pretrained(
        classification_base_model_name,
        num_labels=2,  # ì‹¤ì œ ëª¨ë¸ì´ 2-classë¡œ í•™ìŠµë¨
        trust_remote_code=True,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    )

    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    classification_model = PeftModel.from_pretrained(
        classification_base_model,
        classification_adapter_path
    )
    classification_model = classification_model.to(device)
    classification_model.eval()
    logger.info("âœ… Qwen2.5-7B ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    CLASSIFICATION_AVAILABLE = True
except Exception as e:
    logger.warning(f"âš ï¸ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ë¶„ë¥˜ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    CLASSIFICATION_AVAILABLE = False

# 3. Qwen2.5-14B ì±—ë´‡ ëª¨ë¸ (LoRA ì–´ëŒ‘í„°)
logger.info("ğŸ“¦ Qwen2.5-14B ì±—ë´‡ ëª¨ë¸ ë¡œë”©...")
llm_base_model_name = "Qwen/Qwen2.5-14B-Instruct"
llm_adapter_path = "/workspace/models/qwen-14b"  # Runpodì—ì„œ ëª¨ë¸ ê²½ë¡œ

try:
    # LLM í† í¬ë‚˜ì´ì €
    llm_tokenizer = AutoTokenizer.from_pretrained(
        llm_base_model_name,
        trust_remote_code=True
    )

    # LLM ë² ì´ìŠ¤ ëª¨ë¸
    llm_base_model = AutoModelForCausalLM.from_pretrained(
        llm_base_model_name,
        trust_remote_code=True,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto"
    )

    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    llm_model = PeftModel.from_pretrained(
        llm_base_model,
        llm_adapter_path
    )
    llm_model.eval()
    logger.info("âœ… Qwen2.5-14B ì±—ë´‡ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    LLM_AVAILABLE = True
except Exception as e:
    logger.warning(f"âš ï¸ LLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. LLM ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    LLM_AVAILABLE = False

logger.info("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")


# API ëª¨ë¸
class EmbedRequest(BaseModel):
    text: str
    normalize: bool = True


class EmbedBatchRequest(BaseModel):
    texts: List[str]
    normalize: bool = True


class ClassifyRequest(BaseModel):
    texts: List[str]
    top_k: int = 3  # ìƒìœ„ Kê°œ í´ë˜ìŠ¤ ë°˜í™˜


class LLMRequest(BaseModel):
    prompt: str
    max_length: int = 512
    temperature: float = 0.7
    top_p: float = 0.9


class RAGPipelineRequest(BaseModel):
    """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ìš”ì²­"""
    query: str
    patents: List[dict]  # RAG ê²€ìƒ‰ ê²°ê³¼
    use_classification: bool = True
    max_length: int = 512


# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "device": device,
        "models": {
            "embedding": "BAAI/bge-m3",
            "classification": f"Qwen2.5-7B + LoRA ({'available' if CLASSIFICATION_AVAILABLE else 'unavailable'})",
            "llm": f"Qwen2.5-14B + LoRA ({'available' if LLM_AVAILABLE else 'unavailable'})"
        }
    }


@app.post("/embed")
def embed_text(request: EmbedRequest):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
    try:
        embedding = embedding_model.encode(
            request.text,
            normalize_embeddings=request.normalize,
            show_progress_bar=False
        )
        return {
            "embedding": embedding.tolist(),
            "dimension": len(embedding)
        }
    except Exception as e:
        logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/embed/batch")
def embed_batch(request: EmbedBatchRequest):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë²¡í„°í™”"""
    try:
        embeddings = embedding_model.encode(
            request.texts,
            normalize_embeddings=request.normalize,
            show_progress_bar=False,
            batch_size=32
        )
        return {
            "embeddings": embeddings.tolist(),
            "count": len(embeddings),
            "dimension": embeddings.shape[1]
        }
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/classify")
def classify_patents(request: ClassifyRequest):
    """íŠ¹í—ˆ IPC ë¶„ë¥˜"""
    if not CLASSIFICATION_AVAILABLE:
        raise HTTPException(status_code=503, detail="ë¶„ë¥˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    try:
        results = []

        for text in request.texts:
            # í† í°í™”
            inputs = classification_tokenizer(
                text[:512],  # ìµœëŒ€ 512 í† í°
                return_tensors="pt",
                truncation=True,
                max_length=512
            ).to(device)

            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = classification_model(**inputs)
                logits = outputs.logits

            # ìƒìœ„ Kê°œ í´ë˜ìŠ¤
            probs = torch.softmax(logits, dim=-1)[0]
            top_k_probs, top_k_indices = torch.topk(probs, request.top_k)

            predictions = []
            for prob, idx in zip(top_k_probs.cpu(), top_k_indices.cpu()):
                predictions.append({
                    "class_id": int(idx),
                    "confidence": float(prob)
                })

            results.append({
                "text": text[:100] + "..." if len(text) > 100 else text,
                "predictions": predictions
            })

        return {"classifications": results}

    except Exception as e:
        logger.error(f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/generate")
def generate_response(request: LLMRequest):
    """LLMì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±"""
    if not LLM_AVAILABLE:
        raise HTTPException(status_code=503, detail="LLM ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    try:
        # í† í°í™”
        inputs = llm_tokenizer(
            request.prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(device)

        # ìƒì„±
        with torch.no_grad():
            outputs = llm_model.generate(
                **inputs,
                max_new_tokens=request.max_length,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True,
                pad_token_id=llm_tokenizer.pad_token_id,
                eos_token_id=llm_tokenizer.eos_token_id
            )

        # ë””ì½”ë”©
        response = llm_tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        return {
            "response": response.strip(),
            "prompt_length": inputs['input_ids'].shape[1],
            "generated_length": outputs.shape[1] - inputs['input_ids'].shape[1]
        }

    except Exception as e:
        logger.error(f"ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/rag/pipeline")
def rag_pipeline(request: RAGPipelineRequest):
    """
    ì „ì²´ RAG íŒŒì´í”„ë¼ì¸: ê²€ìƒ‰ â†’ ë¶„ë¥˜ â†’ LLM ë‹µë³€
    """
    try:
        # 1. ë¶„ë¥˜ (ì„ íƒì‚¬í•­)
        classified_patents = []
        if request.use_classification and CLASSIFICATION_AVAILABLE:
            logger.info("íŠ¹í—ˆ ë¶„ë¥˜ ìˆ˜í–‰ ì¤‘...")

            patent_texts = [p.get('text', '')[:512] for p in request.patents]
            classification_result = classify_patents(
                ClassifyRequest(texts=patent_texts, top_k=1)
            )

            for i, patent in enumerate(request.patents):
                patent_with_class = patent.copy()
                patent_with_class['classification'] = classification_result['classifications'][i]
                classified_patents.append(patent_with_class)
        else:
            classified_patents = request.patents

        # 2. LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[íŠ¹í—ˆ {i+1}] {p['application_number']}\n"
            f"ì œëª©: {p.get('title_ko', 'N/A')}\n"
            f"IPC: {p.get('ipc', 'N/A')}\n"
            + (f"ë¶„ë¥˜ ê²°ê³¼: {p.get('classification', {}).get('predictions', [{}])[0].get('class_id', 'N/A')}\n"
               if request.use_classification else "")
            + f"ë‚´ìš©: {p.get('text', '')[:300]}..."
            for i, p in enumerate(classified_patents)
        ])

        prompt = f"""ë‹¤ìŒì€ ê²€ìƒ‰ëœ ê´€ë ¨ íŠ¹í—ˆ ì •ë³´ì…ë‹ˆë‹¤:

{context}

ì‚¬ìš©ì ì§ˆë¬¸: {request.query}

ìœ„ íŠ¹í—ˆ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
íŠ¹í—ˆ ë²ˆí˜¸ì™€ ì œëª©ì„ ì–¸ê¸‰í•˜ë©´ì„œ ëª…í™•í•˜ê²Œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""

        # 3. LLM ë‹µë³€ ìƒì„±
        if LLM_AVAILABLE:
            logger.info("LLM ë‹µë³€ ìƒì„± ì¤‘...")
            llm_response = generate_response(
                LLMRequest(
                    prompt=prompt,
                    max_length=request.max_length
                )
            )

            return {
                "query": request.query,
                "patents_used": len(classified_patents),
                "classified": request.use_classification and CLASSIFICATION_AVAILABLE,
                "response": llm_response['response'],
                "metadata": {
                    "prompt_length": llm_response['prompt_length'],
                    "generated_length": llm_response['generated_length']
                }
            }
        else:
            # LLM ì‚¬ìš© ë¶ˆê°€ ì‹œ ê²€ìƒ‰ ê²°ê³¼ë§Œ ë°˜í™˜
            return {
                "query": request.query,
                "patents_used": len(classified_patents),
                "classified": request.use_classification and CLASSIFICATION_AVAILABLE,
                "response": f"ê´€ë ¨ íŠ¹í—ˆ {len(classified_patents)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n{context}",
                "metadata": {"llm_available": False}
            }

    except Exception as e:
        logger.error(f"RAG íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "gpu_available": torch.cuda.is_available(),
        "device": device,
        "models": {
            "embedding": True,
            "classification": CLASSIFICATION_AVAILABLE,
            "llm": LLM_AVAILABLE
        }
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
