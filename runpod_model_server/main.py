"""
Runpod ëª¨ë¸ ì„œë²„ - FastAPI
BGE-M3 ì„ë² ë”©, Qwen2.5 ë¶„ë¥˜ ëª¨ë¸, Qwen2.5-14B ì±—ë´‡
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM
from peft import PeftModel
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Patent RAG Model Server with Classification and LLM")

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"ğŸ’» Device: {device}")

# ëª¨ë¸ ë¡œë”© (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ)
logger.info("ğŸš€ ëª¨ë¸ ë¡œë”© ì¤‘...")

# 1. BGE-M3 ì„ë² ë”© ëª¨ë¸
logger.info("ğŸ“¦ BGE-M3 ì„ë² ë”© ëª¨ë¸ ë¡œë”©...")
embedding_model = SentenceTransformer('BAAI/bge-m3', device=device)
logger.info("âœ… BGE-M3 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

# 2. Qwen2.5-7B ë¶„ë¥˜ ëª¨ë¸ (LoRA ì–´ëŒ‘í„°)
logger.info("ğŸ“¦ Qwen2.5-7B ë¶„ë¥˜ ëª¨ë¸ ë¡œë”©...")
classification_base_model_name = "Qwen/Qwen2.5-7B-Instruct"
classification_adapter_path = "/workspace/models/classification"  # Runpodì—ì„œ ëª¨ë¸ ê²½ë¡œ

try:
    # ë¶„ë¥˜ ëª¨ë¸ í† í¬ë‚˜ì´ì €
    classification_tokenizer = AutoTokenizer.from_pretrained(
        classification_base_model_name,
        trust_remote_code=True
    )

    # ë¶„ë¥˜ ë² ì´ìŠ¤ ëª¨ë¸
    classification_base_model = AutoModelForSequenceClassification.from_pretrained(
        classification_base_model_name,
        num_labels=2,  # ì‹¤ì œ ëª¨ë¸ì´ 2-classë¡œ í•™ìŠµë¨
        trust_remote_code=True,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32
    )

    # LoRA ì–´ëŒ‘í„° ë¡œë“œ
    classification_model = PeftModel.from_pretrained(
        classification_base_model,
        classification_adapter_path
    )
    classification_model = classification_model.to(device)
    classification_model.eval()
    logger.info("âœ… Qwen2.5-7B ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    CLASSIFICATION_AVAILABLE = True
except Exception as e:
    logger.warning(f"âš ï¸ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ë¶„ë¥˜ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    CLASSIFICATION_AVAILABLE = False

# 3. Qwen2.5-14B ë² ì´ìŠ¤ ëª¨ë¸ (ë“±ë¡ê±´ìš© - íŠœë‹ ì•ˆ ëœ ì›ë³¸)
logger.info("ğŸ“¦ Qwen2.5-14B ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë”© (ë“±ë¡ê±´ìš©)...")
base_model_name = "Qwen/Qwen2.5-14B-Instruct"

try:
    # ë² ì´ìŠ¤ ëª¨ë¸ í† í¬ë‚˜ì´ì €
    base_tokenizer = AutoTokenizer.from_pretrained(
        base_model_name,
        trust_remote_code=True
    )

    # ë² ì´ìŠ¤ ëª¨ë¸ (ë“±ë¡ê±´ìš© - LoRA ì—†ìŒ)
    base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,
        trust_remote_code=True,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        low_cpu_mem_usage=True
    ).to(device)
    base_model.eval()
    logger.info("âœ… Qwen2.5-14B ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë“±ë¡ê±´ìš©)")

    BASE_MODEL_AVAILABLE = True
except Exception as e:
    logger.warning(f"âš ï¸ ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. ë“±ë¡ê±´ ë¶„ì„ ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    BASE_MODEL_AVAILABLE = False

# 4. SLLM (Qwen2.5-14B + qwen-14b LoRA) - ê±°ì ˆ ì´ìœ  ë¶„ì„ ì „ë¬¸ ëª¨ë¸
logger.info("ğŸ“¦ SLLM (ê±°ì ˆ ì´ìœ  ë¶„ì„) ëª¨ë¸ ë¡œë”©...")
sllm_adapter_path = "/workspace/models/qwen-14b"  # checkpoint-16 LoRA ì–´ëŒ‘í„°

try:
    # SLLM í† í¬ë‚˜ì´ì € (ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë™ì¼)
    sllm_tokenizer = base_tokenizer  # ì¬ì‚¬ìš©

    # SLLMìš© ë² ì´ìŠ¤ ëª¨ë¸ ë³„ë„ ë¡œë“œ
    sllm_base_model = AutoModelForCausalLM.from_pretrained(
        base_model_name,  # "Qwen/Qwen2.5-14B-Instruct"
        trust_remote_code=True,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        low_cpu_mem_usage=True
    ).to(device)

    # qwen-14b (checkpoint-16) LoRA ì–´ëŒ‘í„° ë¡œë“œ
    sllm_model = PeftModel.from_pretrained(
        sllm_base_model,
        sllm_adapter_path
    )
    sllm_model.eval()
    logger.info("âœ… SLLM (ê±°ì ˆ ë¶„ì„) ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    SLLM_AVAILABLE = True
except Exception as e:
    logger.warning(f"âš ï¸ SLLM ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}. SLLM ê¸°ëŠ¥ ë¹„í™œì„±í™”")
    SLLM_AVAILABLE = False

logger.info("ğŸ‰ ëª¨ë“  ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")


# API ëª¨ë¸
class EmbedRequest(BaseModel):
    text: str
    normalize: bool = True


class EmbedBatchRequest(BaseModel):
    texts: List[str]
    normalize: bool = True


class ClassifyRequest(BaseModel):
    texts: List[str]
    top_k: int = 3  # ìƒìœ„ Kê°œ í´ë˜ìŠ¤ ë°˜í™˜


class LLMRequest(BaseModel):
    prompt: str
    max_length: int = 512
    temperature: float = 0.7
    top_p: float = 0.9


class RAGPipelineRequest(BaseModel):
    """ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ ìš”ì²­"""
    query: str
    patents: List[dict]  # RAG ê²€ìƒ‰ ê²°ê³¼
    use_classification: bool = True
    max_length: int = 512


# API ì—”ë“œí¬ì¸íŠ¸
@app.get("/")
def root():
    """ì„œë²„ ìƒíƒœ í™•ì¸"""
    return {
        "status": "running",
        "device": device,
        "models": {
            "embedding": "BAAI/bge-m3",
            "classification": f"Qwen2.5-7B + LoRA ({'available' if CLASSIFICATION_AVAILABLE else 'unavailable'})",
            "base_model": f"Qwen2.5-14B Base ({'available' if BASE_MODEL_AVAILABLE else 'unavailable'})",
            "sllm": f"Qwen2.5-14B + SLLM LoRA ({'available' if SLLM_AVAILABLE else 'unavailable'})"
        }
    }


@app.post("/embed")
def embed_text(request: EmbedRequest):
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
    try:
        embedding = embedding_model.encode(
            request.text,
            normalize_embeddings=request.normalize,
            show_progress_bar=False
        )
        return {
            "embedding": embedding.tolist(),
            "dimension": len(embedding)
        }
    except Exception as e:
        logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/embed/batch")
def embed_batch(request: EmbedBatchRequest):
    """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ ë°°ì¹˜ë¡œ ë²¡í„°í™”"""
    try:
        embeddings = embedding_model.encode(
            request.texts,
            normalize_embeddings=request.normalize,
            show_progress_bar=False,
            batch_size=32
        )
        return {
            "embeddings": embeddings.tolist(),
            "count": len(embeddings),
            "dimension": embeddings.shape[1]
        }
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/classify")
def classify_patents(request: ClassifyRequest):
    """íŠ¹í—ˆ IPC ë¶„ë¥˜"""
    if not CLASSIFICATION_AVAILABLE:
        raise HTTPException(status_code=503, detail="ë¶„ë¥˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    try:
        results = []

        for text in request.texts:
            # í† í°í™”
            inputs = classification_tokenizer(
                text[:512],  # ìµœëŒ€ 512 í† í°
                return_tensors="pt",
                truncation=True,
                max_length=512
            ).to(device)

            # ì¶”ë¡ 
            with torch.no_grad():
                outputs = classification_model(**inputs)
                logits = outputs.logits

            # ìƒìœ„ Kê°œ í´ë˜ìŠ¤
            probs = torch.softmax(logits, dim=-1)[0]
            top_k_probs, top_k_indices = torch.topk(probs, request.top_k)

            predictions = []
            for prob, idx in zip(top_k_probs.cpu(), top_k_indices.cpu()):
                class_id = int(idx)
                predictions.append({
                    "class_id": class_id,
                    "label": f"label_{class_id}",  # label_0 (ë“±ë¡) ë˜ëŠ” label_1 (ê±°ì ˆ)
                    "confidence": float(prob)
                })

            results.append({
                "text": text[:100] + "..." if len(text) > 100 else text,
                "predictions": predictions
            })

        return {"classifications": results}

    except Exception as e:
        logger.error(f"ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/generate")
def generate_response(request: LLMRequest):
    """ë² ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„± (ë“±ë¡ê±´ìš©)"""
    if not BASE_MODEL_AVAILABLE:
        raise HTTPException(status_code=503, detail="ë² ì´ìŠ¤ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    try:
        # í† í°í™”
        inputs = base_tokenizer(
            request.prompt,
            return_tensors="pt",
            truncation=True,
            max_length=2048
        ).to(device)

        # ìƒì„±
        with torch.no_grad():
            outputs = base_model.generate(
                **inputs,
                max_new_tokens=request.max_length,
                temperature=request.temperature,
                top_p=request.top_p,
                do_sample=True,
                pad_token_id=base_tokenizer.pad_token_id,
                eos_token_id=base_tokenizer.eos_token_id
            )

        # ë””ì½”ë”©
        response = base_tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        return {
            "response": response.strip(),
            "prompt_length": inputs['input_ids'].shape[1],
            "generated_length": outputs.shape[1] - inputs['input_ids'].shape[1]
        }

    except Exception as e:
        logger.error(f"ìƒì„± ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/rag/pipeline")
def rag_pipeline(request: RAGPipelineRequest):
    """
    ì „ì²´ RAG íŒŒì´í”„ë¼ì¸: ê²€ìƒ‰ â†’ ë¶„ë¥˜ â†’ LLM ë‹µë³€
    """
    try:
        # 1. ë¶„ë¥˜ (ì„ íƒì‚¬í•­)
        classified_patents = []
        is_rejection = False  # ê±°ì ˆ ì—¬ë¶€ í”Œë˜ê·¸

        if request.use_classification and CLASSIFICATION_AVAILABLE:
            logger.info("íŠ¹í—ˆ ë¶„ë¥˜ ìˆ˜í–‰ ì¤‘...")

            patent_texts = [p.get('text', '')[:512] for p in request.patents]
            classification_result = classify_patents(
                ClassifyRequest(texts=patent_texts, top_k=1)
            )

            for i, patent in enumerate(request.patents):
                patent_with_class = patent.copy()
                patent_with_class['classification'] = classification_result['classifications'][i]
                classified_patents.append(patent_with_class)

            # ì²« ë²ˆì§¸ íŠ¹í—ˆì˜ ë¶„ë¥˜ ê²°ê³¼ë¡œ ê±°ì ˆ ì—¬ë¶€ íŒë‹¨
            # ë¶„ë¥˜ ê²°ê³¼: label_0 = ë“±ë¡, label_1 = ê±°ì ˆ (ëª¨ë¸ì— ë”°ë¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ)
            first_classification = classification_result['classifications'][0]['predictions'][0]
            is_rejection = first_classification['label'] == 'label_1'
            logger.info(f"ë¶„ë¥˜ ê²°ê³¼: {'ê±°ì ˆ' if is_rejection else 'ë“±ë¡'} (label: {first_classification['label']})")
        else:
            classified_patents = request.patents

        # 2. ìœ ì‚¬ íŠ¹í—ˆ ëª©ë¡ ìƒì„± (ê³µí†µ)
        similar_claims_text = ""
        mappings = []
        for i, p in enumerate(classified_patents, 1):
            app_no = p.get('application_number', 'N/A')
            title = p.get('title_ko', 'N/A')
            text = p.get('text', '')[:300]
            similar_claims_text += f"{i}) [ì¶œì›ë²ˆí˜¸: {app_no}]\nì œëª©: {title}\në‚´ìš©: {text}...\n\n"
            mappings.append(f"- ì¸ìš©ë°œëª…{i}: ì¶œì›ë²ˆí˜¸ {app_no}")

        # 3. ê±°ì ˆê±´ì´ë©´ SLLM ì‚¬ìš©, ë“±ë¡ê±´ì´ë©´ LLM ì‚¬ìš©
        if is_rejection and SLLM_AVAILABLE:
            logger.info("ğŸ”´ ê±°ì ˆ ê±´ ê°ì§€ â†’ SLLM (checkpoint-16) ì‚¬ìš©")

            # SLLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê±°ì ˆ ì´ìœ  ë¶„ì„ íŠ¹í™”)
            system_msg = (
                "You are Qwen, a helpful patent analysis assistant.\n"
                "ê·œì¹™:\n"
                "1) ë°˜ë“œì‹œ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ê³  ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“± ì™¸êµ­ì–´(í•œì í¬í•¨)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                "2) ì¶œë ¥ì€ ì¤„ë°”ê¿ˆ ì—†ì´ í•œ ë‹¨ë½ì˜ í•œêµ­ì–´ ê³µì‹ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n"
                "3) ë³¸ë¬¸ì—ì„œ ì¸ìš©ë°œëª…ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ 'ì¸ìš©ë°œëª…N(ì¶œì›ë²ˆí˜¸ XXXXX)' í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤.\n"
            )

            # ì„ í–‰ë¬¸í—Œ ì •ë³´ ì¶”ì¶œ (ì²« ë²ˆì§¸ íŠ¹í—ˆë¥¼ ì£¼ ì„ í–‰ë¬¸í—Œìœ¼ë¡œ)
            prior_art_no = classified_patents[0].get('application_number', 'N/A') if classified_patents else 'N/A'

            user_msg = (
                f"ë‹¤ìŒ (ì„ í–‰ë¬¸í—Œ/ìœ ì‚¬ë¬¸ì„œì˜ ì²­êµ¬í•­ ëª©ë¡ê³¼ ëŒ€ìƒ ì²­êµ¬í•­)ì„ ë°”íƒ•ìœ¼ë¡œ, "
                f"ê±°ì ˆ ì‚¬ìœ (ì‹ ê·œì„±, ì§„ë³´ì„±, ëª…í™•ì„± ë“±)ë¥¼ íŒë³„í•˜ê³  í•µì‹¬ ê·¼ê±°ë¥¼ 3ì¤„ ì´ë‚´ë¡œ ê°„ê²°íˆ ì„¤ëª…í•´ì¤˜. "
                f"ìœ ì‚¬ì ê³¼ ì°¨ì´ì ì„ ëª…í™•íˆ ì§€ì í•´.\n\n"
                f"[ì„ í–‰ë¬¸í—Œ/ì¸ìš© ë²ˆí˜¸]\n{prior_art_no}\n\n"
                f"[ëŒ€ìƒ ì²­êµ¬í•­ / ì‚¬ìš©ì ì§ˆë¬¸]\n{request.query}\n\n"
                f"[ìœ ì‚¬ ë¬¸ì„œì˜ ì²­êµ¬í•­ ëª©ë¡ (ìƒìœ„ {len(classified_patents)}ê°œ)]\n{similar_claims_text}"
                f"[ì¸ìš©ë°œëª… ë¼ë²¨-ì¶œì›ë²ˆí˜¸ ë§¤í•‘]\n" + "\n".join(mappings) + "\n\n"
                "ì£¼ì˜: ë³¸ë¬¸ì—ì„œ ì¸ìš©ë°œëª…ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ 'ì¸ìš©ë°œëª…N(ì¶œì›ë²ˆí˜¸ XXXXX)' í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ê³ , "
                "í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ë©° í•œ ë‹¨ë½ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
            )

            prompt = f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant"

            # SLLM ìƒì„± (checkpoint-16 íŒŒë¼ë¯¸í„° ì‚¬ìš©)
            inputs = sllm_tokenizer([prompt], return_tensors="pt", truncation=True, max_length=1792).to(device)

            with torch.inference_mode():
                outputs = sllm_model.generate(
                    **inputs,
                    max_new_tokens=256,
                    do_sample=False,
                    num_beams=3,
                    no_repeat_ngram_size=3,
                    length_penalty=0.9,
                    repetition_penalty=1.1,
                    pad_token_id=sllm_tokenizer.pad_token_id,
                    eos_token_id=sllm_tokenizer.eos_token_id
                )

            response_text = sllm_tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            ).strip()

            # í›„ì²˜ë¦¬: ì¤‘êµ­ì–´ ì œê±°, ì¤„ë°”ê¿ˆ ì œê±°
            import re
            response_text = re.sub(r'[\u4E00-\u9FFF]+', '', response_text)
            response_text = re.sub(r'\s*\n\s*', ' ', response_text).strip()

            # ë§ˆì§€ë§‰ ë¬¸êµ¬ ì¶”ê°€ (ê±°ì ˆ ê²°ë¡ )
            if not response_text.endswith("ë”°ë¼ì„œ íŠ¹í—ˆë¥¼ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."):
                if not response_text.endswith("."):
                    response_text += "."
                response_text += " ë”°ë¼ì„œ íŠ¹í—ˆë¥¼ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            return {
                "query": request.query,
                "patents_used": len(classified_patents),
                "classified": True,
                "classification": "rejection",
                "model_used": "SLLM (checkpoint-16)",
                "response": response_text,
                "metadata": {
                    "prompt_length": inputs['input_ids'].shape[1],
                    "generated_length": outputs.shape[1] - inputs['input_ids'].shape[1]
                }
            }

        # 4. ë“±ë¡ê±´ â†’ ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©
        else:
            logger.info("ğŸŸ¢ ë“±ë¡ ê±´ ê°ì§€ â†’ ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš©")

            # ë“±ë¡ê±´ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_msg = (
                "You are Qwen, a helpful patent analysis assistant.\n"
                "ê·œì¹™:\n"
                "1) ë°˜ë“œì‹œ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ê³  ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“± ì™¸êµ­ì–´(í•œì í¬í•¨)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                "2) ì¶œë ¥ì€ í•œ ë‹¨ë½ì˜ í•œêµ­ì–´ ê³µì‹ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n"
                "3) ìœ ì‚¬ íŠ¹í—ˆì™€ ë¹„êµí•˜ì—¬ ë“±ë¡ ê°€ëŠ¥ì„±ì´ ìˆëŠ” ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.\n"
            )

            user_msg = (
                f"ë‹¤ìŒ ìœ ì‚¬ íŠ¹í—ˆ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì²­êµ¬í•­ì´ ë“±ë¡ëœ íŠ¹í—ˆì„ì„ ë¶„ì„í•´ì£¼ì„¸ìš”.\n\n"
                f"[ì‚¬ìš©ì ì²­êµ¬í•­]\n{request.query}\n\n"
                f"[ìœ ì‚¬ íŠ¹í—ˆ ëª©ë¡ (ìƒìœ„ {len(classified_patents)}ê°œ)]\n{similar_claims_text}\n\n"
                "ìœ„ ìœ ì‚¬ íŠ¹í—ˆë“¤ê³¼ ë¹„êµí–ˆì„ ë•Œ, ì œì¶œëœ ì²­êµ¬í•­ì€ ë“±ë¡ëœ íŠ¹í—ˆì…ë‹ˆë‹¤. "
                "ìœ ì‚¬ íŠ¹í—ˆì™€ì˜ ì°¨ë³„ì ì„ ê°„ë‹¨íˆ ì„¤ëª…í•˜ê³ , ë“±ë¡ ê°€ëŠ¥í•œ ì´ìœ ë¥¼ í•œêµ­ì–´ë¡œ í•œ ë‹¨ë½ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
            )

            prompt = f"<|im_start|>system\n{system_msg}<|im_end|>\n<|im_start|>user\n{user_msg}<|im_end|>\n<|im_start|>assistant"

            if BASE_MODEL_AVAILABLE:
                base_response = generate_response(
                    LLMRequest(
                        prompt=prompt,
                        max_length=request.max_length
                    )
                )

                return {
                    "query": request.query,
                    "patents_used": len(classified_patents),
                    "classified": request.use_classification and CLASSIFICATION_AVAILABLE,
                    "classification": "registration",
                    "model_used": "Base Model (Qwen2.5-14B)",
                    "response": base_response['response'],
                    "metadata": {
                        "prompt_length": base_response['prompt_length'],
                        "generated_length": base_response['generated_length']
                    }
                }
            else:
                # ë² ì´ìŠ¤ ëª¨ë¸ ì‚¬ìš© ë¶ˆê°€ ì‹œ ë‹¨ìˆœ ë©”ì‹œì§€ ë°˜í™˜
                return {
                    "query": request.query,
                    "patents_used": len(classified_patents),
                    "classified": request.use_classification and CLASSIFICATION_AVAILABLE,
                    "classification": "registration",
                    "model_used": "None",
                    "response": f"ì œì¶œí•˜ì‹  ì²­êµ¬í•­ì€ ë“±ë¡ëœ íŠ¹í—ˆë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ìœ ì‚¬ íŠ¹í—ˆ {len(classified_patents)}ê°œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                    "metadata": {"base_model_available": False}
                }

    except Exception as e:
        logger.error(f"RAG íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/health")
def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "healthy",
        "gpu_available": torch.cuda.is_available(),
        "device": device,
        "models": {
            "embedding": True,
            "classification": CLASSIFICATION_AVAILABLE,
            "base_model": BASE_MODEL_AVAILABLE,
            "sllm": SLLM_AVAILABLE
        }
    }


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
