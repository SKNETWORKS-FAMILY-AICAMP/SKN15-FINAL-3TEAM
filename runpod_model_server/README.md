# Runpod ëª¨ë¸ ì„œë²„

**ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ì„ ì„œë¹™í•˜ëŠ” FastAPI ì„œë²„**

- BGE-M3 ì„ë² ë”© ëª¨ë¸
- Qwen2.5-7B ë¶„ë¥˜ ëª¨ë¸ (LoRA)
- Qwen2.5-14B ì±—ë´‡ ëª¨ë¸ (LoRA)

---

## ğŸ“‹ ë°°í¬ ì¤€ë¹„

### 1. í•„ìš”í•œ íŒŒì¼

- `classifiaction.zip` (36MB) - Qwen2.5-7B ë¶„ë¥˜ LoRA ì–´ëŒ‘í„°
- `Qwen-14B-checkpoint-16-20251114T001419Z-1-001.zip` (43MB) - Qwen2.5-14B ì±—ë´‡ LoRA ì–´ëŒ‘í„°

### 2. Runpod GPU Pod ìƒì„±

1. [Runpod](https://www.runpod.io/) ë¡œê·¸ì¸
2. "Deploy" â†’ "GPU Pods" ì„ íƒ
3. **GPU ì„ íƒ (ì¤‘ìš”!):**
   - **ê¶Œì¥: RTX A5000** (24GB VRAM, ~$0.34/hr)
   - ë˜ëŠ”: **RTX A6000** (48GB VRAM, ~$0.79/hr)
   - ìµœì†Œ: **RTX 3090** (24GB VRAM, ~$0.34/hr)

   > **âš ï¸ ì¤‘ìš”:** Qwen2.5-14BëŠ” ì•½ 15-20GB VRAM í•„ìš”, ë¶„ë¥˜ ëª¨ë¸ì€ ì•½ 7-10GB í•„ìš”
   > ì´ VRAM: ìµœì†Œ 24GB ì´ìƒ (RTX 3090, A5000, A6000)

4. Template: **PyTorch 2.0+** ì„ íƒ
5. Disk: **100GB** ì´ìƒ
6. "Deploy" í´ë¦­

---

## ğŸš€ Runpod ì„œë²„ ë°°í¬

### Step 1: Pod ì ‘ì†

Runpod ì›¹ í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰:

```bash
# ì €ì¥ì†Œ í´ë¡ 
git clone https://github.com/your-repo/SKN15-FINAL-3TEAM.git
cd SKN15-FINAL-3TEAM/runpod_model_server
```

### Step 2: ëª¨ë¸ íŒŒì¼ ì—…ë¡œë“œ

```bash
# ë¡œì»¬ì—ì„œ Runpodë¡œ ëª¨ë¸ íŒŒì¼ ì „ì†¡
# ë°©ë²• 1: SCP (Runpod SSH ì‚¬ìš©)
scp -P <RUNPOD_SSH_PORT> \
  /path/to/classifiaction.zip \
  root@<RUNPOD_IP>:/workspace/

scp -P <RUNPOD_SSH_PORT> \
  "/path/to/Qwen-14B-checkpoint-16-20251114T001419Z-1-001.zip" \
  root@<RUNPOD_IP>:/workspace/

# ë°©ë²• 2: ì§ì ‘ ë‹¤ìš´ë¡œë“œ (Google Drive/Dropbox ë§í¬)
cd /workspace
wget "YOUR_GOOGLE_DRIVE_LINK" -O classifiaction.zip
wget "YOUR_GOOGLE_DRIVE_LINK" -O qwen-14b.zip
```

### Step 3: ëª¨ë¸ ì••ì¶• í•´ì œ ë° ë°°ì¹˜

```bash
cd /workspace

# ë¶„ë¥˜ ëª¨ë¸ ì••ì¶• í•´ì œ
mkdir -p models/classification
unzip classifiaction.zip -d models/classification

# ì±—ë´‡ ëª¨ë¸ ì••ì¶• í•´ì œ
mkdir -p models/qwen-14b-temp
unzip "Qwen-14B-checkpoint-16-20251114T001419Z-1-001.zip" -d models/qwen-14b-temp
mv models/qwen-14b-temp/Qwen-14B-checkpoint-16/* models/qwen-14b/

# í™•ì¸
ls -lh models/classification/
ls -lh models/qwen-14b/
```

### Step 4: íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
cd /workspace/SKN15-FINAL-3TEAM/runpod_model_server

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install -r requirements.txt
```

### Step 5: ì„œë²„ ì‹¤í–‰

```bash
# ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰
nohup python main.py > model_server.log 2>&1 &

# ë¡œê·¸ í™•ì¸
tail -f model_server.log

# ë˜ëŠ” í¬ê·¸ë¼ìš´ë“œ ì‹¤í–‰ (ë””ë²„ê¹…ìš©)
python main.py
```

### Step 6: ê³µê°œ URL í™•ì¸

1. Runpod ëŒ€ì‹œë³´ë“œì—ì„œ Pod ì„ íƒ
2. "TCP Port Mappings" ì„¹ì…˜ í™•ì¸
3. Port `8001`ì˜ ê³µê°œ URL ë³µì‚¬
   - ì˜ˆ: `https://abc123-8001.proxy.runpod.net`

---

## âœ… í…ŒìŠ¤íŠ¸

### 1. í—¬ìŠ¤ ì²´í¬

```bash
curl https://abc123-8001.proxy.runpod.net/health
```

**ì˜ˆìƒ ì‘ë‹µ:**
```json
{
  "status": "healthy",
  "gpu_available": true,
  "device": "cuda",
  "models": {
    "embedding": true,
    "classification": true,
    "llm": true
  }
}
```

### 2. ì„ë² ë”© í…ŒìŠ¤íŠ¸

```bash
curl -X POST https://abc123-8001.proxy.runpod.net/embed \
  -H "Content-Type: application/json" \
  -d '{"text": "ìë™ì°¨ ê´€ë ¨ íŠ¹í—ˆ"}'
```

### 3. ë¶„ë¥˜ í…ŒìŠ¤íŠ¸

```bash
curl -X POST https://abc123-8001.proxy.runpod.net/classify \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["ìë™ì°¨ ìë™ ë³€ì† ì¥ì¹˜ì— ê´€í•œ ë°œëª…"],
    "top_k": 3
  }'
```

### 4. LLM ìƒì„± í…ŒìŠ¤íŠ¸

```bash
curl -X POST https://abc123-8001.proxy.runpod.net/generate \
  -H "Content-Type: application/json" \
  -d '{
    "prompt": "ìë™ì°¨ íŠ¹í—ˆì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.",
    "max_length": 200
  }'
```

### 5. ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸

```bash
curl -X POST https://abc123-8001.proxy.runpod.net/rag/pipeline \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ìë™ì°¨ ê´€ë ¨ íŠ¹í—ˆ ì°¾ì•„ì¤˜",
    "patents": [
      {
        "application_number": "1019830003182",
        "title_ko": "ìë™ì‹¤ë°©ìš¸ê°€ê³µì¥ì¹˜",
        "ipc": "A63H 3/44",
        "text": "ë³¸ë¬¸ ë‚´ìš©..."
      }
    ],
    "use_classification": true,
    "max_length": 512
  }'
```

---

## ğŸ“Š ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

### GPU ì‚¬ìš©ë¥ 

```bash
# Runpod ì›¹ í„°ë¯¸ë„ì—ì„œ
nvidia-smi

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
watch -n 1 nvidia-smi
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

```bash
# GPU ë©”ëª¨ë¦¬
nvidia-smi --query-gpu=memory.used,memory.total --format=csv

# ì‹œìŠ¤í…œ ë©”ëª¨ë¦¬
free -h
```

### ì„œë²„ ë¡œê·¸

```bash
tail -f /workspace/SKN15-FINAL-3TEAM/runpod_model_server/model_server.log
```

---

## ğŸ”§ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: CUDA Out of Memory

**ì¦ìƒ:**
```
RuntimeError: CUDA out of memory
```

**í•´ê²°:**
1. ë” í° VRAMì˜ GPU ì‚¬ìš© (A6000 48GB)
2. ë˜ëŠ” 8ë¹„íŠ¸ ì–‘ìí™” ì‚¬ìš©:
   ```python
   # main.py ìˆ˜ì •
   llm_base_model = AutoModelForCausalLM.from_pretrained(
       llm_base_model_name,
       load_in_8bit=True,  # 8ë¹„íŠ¸ ì–‘ìí™”
       device_map="auto"
   )
   ```

### ë¬¸ì œ 2: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨

**ì¦ìƒ:**
```
âš ï¸ ë¶„ë¥˜ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: [Errno 2] No such file or directory
```

**í•´ê²°:**
ëª¨ë¸ ê²½ë¡œ í™•ì¸:
```bash
ls -lh /workspace/models/classification/
ls -lh /workspace/models/qwen-14b/
```

### ë¬¸ì œ 3: ì„œë²„ ì‘ë‹µ ëŠë¦¼

**ì›ì¸:** GPUê°€ ì•„ë‹Œ CPUì—ì„œ ì‹¤í–‰ ì¤‘

**í•´ê²°:**
```bash
# GPU ì‚¬ìš© ê°€ëŠ¥ í™•ì¸
python -c "import torch; print(torch.cuda.is_available())"
# Trueì—¬ì•¼ í•¨

# CUDA ë²„ì „ í™•ì¸
nvidia-smi
```

---

## ğŸ’¡ ìµœì í™” íŒ

### 1. ë°°ì¹˜ ì¶”ë¡ 

ì—¬ëŸ¬ íŠ¹í—ˆë¥¼ í•œ ë²ˆì— ë¶„ë¥˜:
```python
# Djangoì—ì„œ
patent_texts = [p['text'] for p in patents[:10]]  # 10ê°œì”© ë°°ì¹˜
response = requests.post(
    f"{MODEL_SERVER_URL}/classify",
    json={"texts": patent_texts, "top_k": 1}
)
```

### 2. ìºì‹±

ìì£¼ ê²€ìƒ‰ë˜ëŠ” ì¿¼ë¦¬ëŠ” Redisì— ìºì‹±:
```python
# Djangoì—ì„œ
import redis
r = redis.Redis()

cache_key = f"rag:{query_hash}"
cached = r.get(cache_key)
if cached:
    return json.loads(cached)
```

### 3. ë¹„ìš© ì ˆê°

- **Spot Instance** ì‚¬ìš© (ìµœëŒ€ 70% í• ì¸)
- ì‚¬ìš©í•˜ì§€ ì•Šì„ ë•Œ Pod ì •ì§€
- ìë™ ìŠ¤ì¼€ì¼ë§ ì„¤ì •

---

## ğŸ“ API ì—”ë“œí¬ì¸íŠ¸

### GET /
ì„œë²„ ìƒíƒœ í™•ì¸

### POST /embed
ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”©

### POST /embed/batch
ë°°ì¹˜ ì„ë² ë”©

### POST /classify
íŠ¹í—ˆ ë¶„ë¥˜

### POST /generate
LLM í…ìŠ¤íŠ¸ ìƒì„±

### POST /rag/pipeline
**ì „ì²´ RAG íŒŒì´í”„ë¼ì¸ (ë©”ì¸ ì—”ë“œí¬ì¸íŠ¸)**

**ìš”ì²­:**
```json
{
  "query": "ì‚¬ìš©ì ì§ˆë¬¸",
  "patents": [ê²€ìƒ‰ëœ íŠ¹í—ˆ ë¦¬ìŠ¤íŠ¸],
  "use_classification": true,
  "max_length": 512
}
```

**ì‘ë‹µ:**
```json
{
  "query": "ì‚¬ìš©ì ì§ˆë¬¸",
  "patents_used": 3,
  "classified": true,
  "response": "LLMì´ ìƒì„±í•œ ë‹µë³€",
  "metadata": {
    "prompt_length": 150,
    "generated_length": 300
  }
}
```

### GET /health
í—¬ìŠ¤ ì²´í¬

---

**ë¬¸ì˜:** GitHub Issues ë˜ëŠ” Runpod ì»¤ë®¤ë‹ˆí‹°
