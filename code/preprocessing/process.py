# âœ… ì„¤ì¹˜ (ì²˜ìŒ í•œ ë²ˆ)
# %pip -q install PyPDF2 pdfminer.six pandas

from pathlib import Path
from typing import List, Union
import pandas as pd
import re, unicodedata

# ============================
# 1) PDF â†’ pages.csv (ë¬¸ì„œë³„ 'ë§ˆì§€ë§‰ í˜ì´ì§€ ì œì™¸' ì €ì¥)
# ============================

def extract_text_per_page(pdf_path: Union[str, Path], skip_last_page: bool = True) -> List[str]:
    """PyPDF2 ë¨¼ì € ì‹œë„ â†’ ì‹¤íŒ¨/ë¹ˆë¬¸ìë©´ pdfminer.sixë¡œ í´ë°±. ë§ˆì§€ë§‰ í˜ì´ì§€ëŠ” ì„ íƒì ìœ¼ë¡œ ì œì™¸."""
    pdf_path = str(pdf_path)
    pages = []
    # 1) PyPDF2
    try:
        from PyPDF2 import PdfReader
        reader = PdfReader(pdf_path)
        for p in reader.pages:
            pages.append(p.extract_text() or "")
        if any(pages):   # í•œ í˜ì´ì§€ë§Œì´ë¼ë„ ì¶”ì¶œë˜ë©´ ì‚¬ìš©
            return pages[:-1] if (skip_last_page and len(pages) > 0) else pages
    except Exception:
        pass
    # 2) pdfminer.six (form-feedë¡œ í˜ì´ì§€ ë¶„ë¦¬)
    from pdfminer.high_level import extract_text
    full = extract_text(pdf_path) or ""
    parts = re.split(r"\f+", full)
    parts = [p.strip("\n") for p in parts if p is not None]
    if skip_last_page and len(parts) > 0:
        parts = parts[:-1]
    return parts

def convert_pdf_to_pages_csv(src: Union[str, Path], out_dir: Union[str, Path], skip_last_page: bool = True):
    """ë‹¨ì¼ PDF â†’ pages.csv (append)"""
    src = Path(src)
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    pages_csv = out_dir / "pages.csv"

    pages = extract_text_per_page(src, skip_last_page=skip_last_page)
    rows = [{"file_name": src.name, "page": i+1, "text": t} for i, t in enumerate(pages)]
    df = pd.DataFrame(rows, columns=["file_name", "page", "text"])

    if pages_csv.exists():
        df.to_csv(pages_csv, index=False, mode="a", header=False, encoding="utf-8-sig")
    else:
        df.to_csv(pages_csv, index=False, encoding="utf-8-sig")

def batch_convert_pages(input_path: Union[str, Path], out_dir: Union[str, Path], skip_last_page: bool = True):
    """íŒŒì¼ì´ë©´ ê·¸ íŒŒì¼ í•˜ë‚˜, í´ë”ë©´ í•˜ìœ„ ëª¨ë“  PDFë¥¼ pages.csvë¡œ ëˆ„ì  ì €ì¥."""
    input_path = Path(input_path)
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)
    pages_csv = out_dir / "pages.csv"
    if pages_csv.exists():
        pages_csv.unlink()  # ìƒˆë¡œ ì‹œì‘

    pdfs = [input_path] if input_path.is_file() else list(input_path.rglob("*.pdf"))
    for pdf in pdfs:
        try:
            convert_pdf_to_pages_csv(pdf, out_dir, skip_last_page=skip_last_page)
        except Exception as e:
            print(f"âŒ Failed: {pdf} ({e})")

# ============================
# 2) pages.csv â†’ docs.csv (ë¬¸ì„œë³„ 1í–‰ìœ¼ë¡œ í•©ì¹˜ê¸°: ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰)
# ============================

# ---------- ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ íŒ¨í„´ ----------
BOILER_PATTERNS = [
    r"^\s*\d{2}-\d{4}-\d{7}\s*$",      # 10-2020-0070641 í˜•íƒœ(ì¶œì›ë²ˆí˜¸)
    r"^\s*-+\s*\d+\s*-+\s*$",          # - 1 - (í˜ì´ì§€ ë„˜ë²„)
    r"^\s*Page\s+\d+\s+of\s+\d+\s*$",
    r"^\s*ë°œ\s*ì†¡\s*ë²ˆ\s*í˜¸.*$",
    r"^\s*ë°œ\s*ì†¡\s*ì¼\s*ì.*$",
    r"^\s*ì œ\s*ì¶œ\s*ê¸°\s*ì¼.*$",
    r"^\s*YOUR INVENTION PARTNER\s*$",
    r"^\s*\[.*ì„œì‹.*\]\s*$",
]
BOILER_RX = re.compile("|".join(BOILER_PATTERNS))

# ---------- ì •ë¦¬ í•¨ìˆ˜ ----------
def normalize_text(s: str) -> str:
    """ìœ ë‹ˆì½”ë“œ/ê³µë°± ì •ë¦¬"""
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", str(s))
    s = s.replace("\u00A0", " ")                 # nbsp â†’ space
    s = re.sub(r"[ \t]{2,}", " ", s)             # ì—°ì† ê³µë°± ì¶•ì†Œ
    s = "\n".join(ln.rstrip() for ln in s.splitlines())  # í–‰ ë ê³µë°± ì œê±°
    return s

# ë¬¸ì¥ë¶€í˜¸/ë‹¤ìŒ ì¤„ ì‹œì‘ë¬¸ì ê¸°ë°˜ ê°œí–‰ ë³´ì •(ë³´ìˆ˜ì )
TERM = re.compile(r"[\.ã€‚!?ï¼ï¼Ÿâ€¦)]$")         # ì´ì „ ì¤„ì´ ë¬¸ì¥ë¶€í˜¸ë¡œ ëë‚˜ëŠ”ê°€
START_OK = re.compile(r"^[ê°€-í£A-Za-z0-9\(]")  # ë‹¤ìŒ ì¤„ì´ ìì—°ìŠ¤ëŸ¬ìš´ ì‹œì‘ì¸ê°€

def join_broken_lines(s: str) -> str:
    lines = s.splitlines()
    if not lines:
        return s
    out = [lines[0]]
    for ln in lines[1:]:
        prev = out[-1]
        if prev and not TERM.search(prev.strip()) and START_OK.search(ln.strip()):
            out[-1] = (prev.rstrip() + " " + ln.lstrip()).strip()
        else:
            out.append(ln)
    return "\n".join(out)

def remove_boilerplate(text: str) -> str:
    """ë¨¸ë¦¬ë§/ê¼¬ë¦¬ë§/í…œí”Œë¦¿ ë¼ì¸ ì œê±° (ë¼ì¸ ë‹¨ìœ„ í•„í„°)"""
    kept = []
    for ln in (text or "").splitlines():
        if not BOILER_RX.search(ln):
            kept.append(ln)
    return "\n".join(kept)

def clean_page_text(raw: str) -> str:
    t = normalize_text(raw)
    t = remove_boilerplate(t)
    t = join_broken_lines(t)
    # ê³¼ë„í•œ ë¹ˆ ì¤„ ì¶•ì†Œ
    t = re.sub(r"\n{3,}", "\n\n", t).strip()
    return t

def pages_to_docs(input_csv: Union[str, Path], out_csv: Union[str, Path], page_sep: str = "\n\n"):
    """pages.csv â†’ docs.csv (file_name, full_text)"""
    in_path = Path(input_csv)
    df = pd.read_csv(in_path)

    # ì»¬ëŸ¼ëª… ì¶”ì •(ì†Œë¬¸ìë¡œ ëŒ€ì¡°)
    cols_lower = {c.lower(): c for c in df.columns}
    file_col = cols_lower.get("file_name") or cols_lower.get("filename") or df.columns[0]
    page_col = cols_lower.get("page") or df.columns[1]
    text_col = cols_lower.get("text") or df.columns[-1]

    # í˜ì´ì§€ ì •ë ¬(ìˆ«ì ë³€í™˜ ì‹œë„ í›„ ì‹¤íŒ¨í•˜ë©´ ë¬¸ìì—´ ì •ë ¬)
    def _to_int_safe(x):
        try:
            return int(x)
        except Exception:
            return x
    df["_page_sort"] = df[page_col].apply(_to_int_safe)
    df = df.sort_values([file_col, "_page_sort"]).drop(columns=["_page_sort"])

    # í˜ì´ì§€ë³„ ì •ì œ í…ìŠ¤íŠ¸ ìƒì„±
    df["text_clean"] = df[text_col].apply(clean_page_text)

    # íŒŒì¼(ë¬¸ì„œ)ë³„ í˜ì´ì§€ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    docs = (
        df.groupby(file_col, sort=False)["text_clean"]
          .apply(lambda parts: page_sep.join([p for p in parts if isinstance(p, str) and p.strip()]))
          .reset_index()
          .rename(columns={file_col: "file_name", "text_clean": "full_text"})
    )

    # ê²°ê³¼ ì €ì¥
    out_path = Path(out_csv)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    docs.to_csv(out_path, index=False, encoding="utf-8-sig")

    print("âœ… saved:", out_path)
    print("rows:", len(docs))

# ============================
# ğŸ” ì‹¤í–‰ ì˜ˆì‹œ
# ============================
# 1) PDF â†’ pages.csv (ë§ˆì§€ë§‰ í˜ì´ì§€ ì œì™¸)
PDF_INPUT = "/home/laydata/workspace/final_project/opinion/file/opinion_pdfs"  # â† í´ë”(ë˜ëŠ” ë‹¨ì¼ íŒŒì¼ ê°€ëŠ¥)
INPUT_DIR = "/home/laydata/workspace/final_project/opinion"                      # pages.csvê°€ ì €ì¥ë  í´ë”
OUT_CSV   = "/home/laydata/workspace/final_project/opinion/opinion_decision_merged.csv"      # ìµœì¢… ì¶œë ¥

# 1) PDF â†’ pages.csv (ë§ˆì§€ë§‰ í˜ì´ì§€ ì œì™¸)
batch_convert_pages(
    PDF_INPUT,        # í´ë” ë˜ëŠ” ë‹¨ì¼ PDF íŒŒì¼ ê²½ë¡œ
    INPUT_DIR,        # pages.csv ì €ì¥ í´ë”
    skip_last_page=True
)

# 2) pages.csv â†’ docs_merged.csv (ì „ì²˜ë¦¬ë§Œ, ê¼¬ë¦¬ ìë¥´ê¸° ì—†ìŒ)
pages_to_docs(
    str(Path(INPUT_DIR) / "pages.csv"),
    OUT_CSV,
    page_sep="\n\n"
)