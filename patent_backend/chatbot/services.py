"""
ì±—ë´‡ ì„œë¹„ìŠ¤ ë ˆì´ì–´ - ëª¨ë¸ êµì²´ê°€ ìš©ì´í•œ ì¶”ìƒí™” êµ¬ì¡°
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Optional
import requests
import logging
import re
import os
import pandas as pd
from difflib import SequenceMatcher
from django.conf import settings
from .lunch_data import get_all_menu_items, get_random_menu, get_menu_by_category, get_random_menu_by_category, LUNCH_MENU
from .rag_service import RAGService

logger = logging.getLogger(__name__)

# CSV íŒŒì¼ ê²½ë¡œ (í•˜ë“œì½”ë”©ëœ ë°ì´í„°)
CSV_FILE_PATH = os.path.join(os.path.dirname(__file__), 'infer_checkpoint-16_full_scored.csv')

# CSV ë°ì´í„° ë¡œë”© (ì„œë²„ ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë”©)
_hardcoded_df = None

def load_hardcoded_data():
    """CSV íŒŒì¼ì„ í•œ ë²ˆë§Œ ë¡œë”©í•˜ì—¬ ë©”ëª¨ë¦¬ì— ìºì‹±"""
    global _hardcoded_df
    if _hardcoded_df is None:
        try:
            _hardcoded_df = pd.read_csv(CSV_FILE_PATH)
            logger.info(f"âœ… í•˜ë“œì½”ë”© ë°ì´í„° ë¡œë”© ì™„ë£Œ: {len(_hardcoded_df)}ê°œ ì²­êµ¬í•­")
        except Exception as e:
            logger.error(f"âŒ CSV ë¡œë”© ì‹¤íŒ¨: {e}")
            _hardcoded_df = pd.DataFrame()  # ë¹ˆ ë°ì´í„°í”„ë ˆì„
    return _hardcoded_df


def find_matching_claim(query: str, threshold: float = 0.8) -> Optional[str]:
    """
    ì‚¬ìš©ì ì¿¼ë¦¬ì™€ ê°€ì¥ ìœ ì‚¬í•œ claim_textë¥¼ ì°¾ì•„ì„œ pred_checkpoint16 ë°˜í™˜

    Args:
        query: ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸
        threshold: ìœ ì‚¬ë„ ì„ê³„ê°’ (0.8 = 80% ìœ ì‚¬ë„)

    Returns:
        pred_checkpoint16 í…ìŠ¤íŠ¸ ë˜ëŠ” None
    """
    df = load_hardcoded_data()

    if df.empty:
        return None

    # ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í•­ëª© ì°¾ê¸°
    exact_match = df[df['claim_text'] == query]
    if not exact_match.empty:
        return exact_match.iloc[0]['pred_checkpoint16']

    # ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
    best_match = None
    best_score = 0.0

    for idx, row in df.iterrows():
        claim_text = row['claim_text']
        score = SequenceMatcher(None, query, claim_text).ratio()

        if score > best_score:
            best_score = score
            best_match = row

    # ì„ê³„ê°’ ì´ìƒì¼ ë•Œë§Œ ë°˜í™˜
    if best_score >= threshold:
        logger.info(f"ğŸ“Š í•˜ë“œì½”ë”© ë§¤ì¹­: ìœ ì‚¬ë„ {best_score:.2%}")
        return best_match['pred_checkpoint16']

    return None


def detect_lunch_request(message: str) -> tuple[bool, str]:
    """
    ì ì‹¬ ë©”ë‰´ ì¶”ì²œ ìš”ì²­ì¸ì§€ ê°ì§€

    Returns:
        (is_lunch_request, category): ì ì‹¬ ìš”ì²­ ì—¬ë¶€ì™€ ì¹´í…Œê³ ë¦¬
    """
    message_lower = message.lower()

    # ì ì‹¬ ê´€ë ¨ í‚¤ì›Œë“œ
    lunch_keywords = ['ì ì‹¬', 'ë©”ë‰´', 'ë­ ë¨¹', 'ë­ë¨¹', 'ë¨¹ì„ê¹Œ', 'ì‹ì‚¬', 'ë°¥', 'ì €ë…']

    # ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ
    category_map = {
        'í•œì‹': ['í•œì‹', 'í•œêµ­', 'ê¹€ì¹˜', 'ëœì¥', 'ë¹„ë¹”ë°¥', 'ì œìœ¡'],
        'ì¤‘ì‹': ['ì¤‘ì‹', 'ì¤‘êµ­', 'ì§œì¥', 'ì§¬ë½•', 'íƒ•ìˆ˜ìœ¡'],
        'ì¼ì‹': ['ì¼ì‹', 'ì¼ë³¸', 'ì´ˆë°¥', 'íšŒ', 'ë¼ë©˜', 'ìš°ë™', 'ëˆì¹´ì¸ '],
        'ì–‘ì‹': ['ì–‘ì‹', 'íŒŒìŠ¤íƒ€', 'í”¼ì', 'ìŠ¤í…Œì´í¬', 'í–„ë²„ê±°'],
        'ë¶„ì‹': ['ë¶„ì‹', 'ë–¡ë³¶ì´', 'ë¼ë©´', 'ê¹€ë°¥'],
    }

    # ì ì‹¬ ìš”ì²­ì¸ì§€ í™•ì¸
    is_lunch = any(keyword in message_lower for keyword in lunch_keywords)

    if not is_lunch:
        return False, None

    # ì¹´í…Œê³ ë¦¬ ê°ì§€
    for category, keywords in category_map.items():
        if any(keyword in message_lower for keyword in keywords):
            return True, category

    return True, None  # ì¹´í…Œê³ ë¦¬ ì—†ì´ ì ì‹¬ ìš”ì²­


class BaseChatService(ABC):
    """ì±—ë´‡ ëª¨ë¸ ì¸í„°í˜ì´ìŠ¤ - ëª¨ë“  ëª¨ë¸ì€ ì´ ì¸í„°í˜ì´ìŠ¤ë¥¼ êµ¬í˜„í•´ì•¼ í•¨"""

    @abstractmethod
    def generate_response(self, message: str, file_content: Optional[str] = None,
                         conversation_history: Optional[List[Dict]] = None) -> str:
        """
        ì‚¬ìš©ì ë©”ì‹œì§€ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            file_content: ì—…ë¡œë“œëœ íŒŒì¼ ë‚´ìš© (ì„ íƒ)
            conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­ (ì„ íƒ)

        Returns:
            AI ì‘ë‹µ í…ìŠ¤íŠ¸
        """
        pass


class LlamaChatService(BaseChatService):
    """LLaMA 8B ëª¨ë¸ ì„œë¹™ ì„œë²„ë¥¼ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.model_server_url = getattr(
            settings,
            'MODEL_SERVER_URL',
            'http://localhost:8001'
        )

    def generate_response(self, message: str, file_content: Optional[str] = None,
                         conversation_history: Optional[List[Dict]] = None) -> str:
        """LLaMA ëª¨ë¸ ì„œë²„ì— ìš”ì²­í•˜ì—¬ ì‘ë‹µ ìƒì„±"""

        # ì ì‹¬ ë©”ë‰´ ìš”ì²­ì¸ì§€ ë¨¼ì € í™•ì¸
        is_lunch, category = detect_lunch_request(message)

        if is_lunch:
            if category:
                # íŠ¹ì • ì¹´í…Œê³ ë¦¬ ìš”ì²­
                menus = get_random_menu_by_category(category, 3)
                return f"ì˜¤ëŠ˜ {category} ë©”ë‰´ ì¶”ì²œë“œë¦½ë‹ˆë‹¤:\n" + "\n".join(f"â€¢ {menu}" for menu in menus)
            else:
                # ì „ì²´ ë©”ë‰´ì—ì„œ ëœë¤
                menus = get_random_menu(3)
                return f"ì˜¤ëŠ˜ì˜ ì ì‹¬ ë©”ë‰´ ì¶”ì²œë“œë¦½ë‹ˆë‹¤:\n" + "\n".join(f"â€¢ {menu}" for menu in menus)

        try:
            # Qwen Chat Template í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            system_msg = "You are a helpful assistant. Answer concisely in Korean. Do not repeat the question or add unnecessary explanations."

            # ëŒ€í™” ê¸°ë¡ í•„í„°ë§
            recent_history = []
            if conversation_history and len(conversation_history) > 0:
                for msg in conversation_history[-6:]:
                    content = msg.get('content', '')
                    # ì ì‹¬ ë©”ë‰´, ì—ëŸ¬ ë©”ì‹œì§€ ì œì™¸
                    if ('ì ì‹¬' not in content and 'ë©”ë‰´' not in content and
                        not content.startswith('ëª¨ë¸ ì„œë²„ ì˜¤ë¥˜') and
                        not content.startswith('ì˜¤ëŠ˜') and
                        'ë¨¹' not in content):
                        recent_history.append(msg)
                recent_history = recent_history[-3:]  # ìµœê·¼ 3ê°œë§Œ

            # Qwen Chat Template êµ¬ì„±
            prompt_parts = [f"<|im_start|>system\n{system_msg}<|im_end|>"]

            # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
            for msg in recent_history:
                role = "user" if msg['type'] == 'user' else "assistant"
                prompt_parts.append(f"<|im_start|>{role}\n{msg['content']}<|im_end|>")

            # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
            prompt_parts.append(f"<|im_start|>user\n{message}<|im_end|>")
            prompt_parts.append("<|im_start|>assistant")

            prompt = "\n".join(prompt_parts)

            # ëª¨ë¸ ì„œë²„ì— POST ìš”ì²­
            response = requests.post(
                f"{self.model_server_url}/generate",
                json={
                    "prompt": prompt,
                    "max_length": 128,
                    "temperature": 0.7
                },
                timeout=300
            )

            if response.status_code == 200:
                data = response.json()
                return data.get("response", "ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                return f"ëª¨ë¸ ì„œë²„ ì˜¤ë¥˜ (HTTP {response.status_code}): {response.text}"

        except requests.exceptions.ConnectionError:
            return "ëª¨ë¸ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        except requests.exceptions.Timeout:
            return "ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except Exception as e:
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


# KoSBERTChatService í´ë˜ìŠ¤ ì œê±°ë¨
# ì™¸ë¶€ ëª¨ë¸ ì„œë²„ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ LlamaChatService ë˜ëŠ” CustomModelChatServiceë¥¼ ì‚¬ìš©í•˜ì„¸ìš”


class CustomModelChatService(BaseChatService):
    """ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡ ì„œë¹„ìŠ¤"""

    def __init__(self):
        self.model_server_url = getattr(
            settings,
            'CUSTOM_MODEL_SERVER_URL',
            'http://localhost:8002'
        )

    def _build_prompt_with_history(self, message: str, conversation_history: Optional[List[Dict]] = None) -> str:
        """
        ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±

        Args:
            message: í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
            conversation_history: ì´ì „ ëŒ€í™” ë‚´ì—­

        Returns:
            ë©€í‹°í„´ ì»¨í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ í”„ë¡¬í”„íŠ¸
        """
        if not conversation_history:
            return message

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í”„ë¡¬í”„íŠ¸ì— í¬í•¨
        prompt_parts = ["ì´ì „ ëŒ€í™” ë‚´ì—­:"]
        for hist in conversation_history[-10:]:  # ìµœê·¼ 10ê°œ ëŒ€í™”ë§Œ ì‚¬ìš©
            role = "ì‚¬ìš©ì" if hist['type'] == 'user' else "AI"
            prompt_parts.append(f"{role}: {hist['content']}")

        prompt_parts.append(f"\ní˜„ì¬ ì§ˆë¬¸:\nì‚¬ìš©ì: {message}")
        prompt_parts.append("\nAI:")

        return "\n".join(prompt_parts)

    def generate_response(self, message: str, file_content: Optional[str] = None,
                         conversation_history: Optional[List[Dict]] = None) -> str:
        """ì»¤ìŠ¤í…€ ëª¨ë¸ ì„œë²„ì— ìš”ì²­í•˜ì—¬ ì‘ë‹µ ìƒì„± (ë©€í‹°í„´ ì§€ì›)"""

        try:
            # ë©€í‹°í„´ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._build_prompt_with_history(message, conversation_history)

            # ëª¨ë¸ ì„œë²„ì— POST ìš”ì²­
            response = requests.post(
                f"{self.model_server_url}/generate",
                json={
                    "prompt": prompt,
                    "message": message,  # ì›ë³¸ ë©”ì‹œì§€ë„ ì „ë‹¬
                    "file_content": file_content,
                    "conversation_history": conversation_history,
                    "max_tokens": 512,
                    "temperature": 0.7
                },
                timeout=60
            )

            if response.status_code == 200:
                data = response.json()
                return data.get("response", "ì‘ë‹µì„ ë°›ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                return f"ëª¨ë¸ ì„œë²„ ì˜¤ë¥˜ (HTTP {response.status_code}): {response.text}"

        except requests.exceptions.ConnectionError:
            return "ëª¨ë¸ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
        except requests.exceptions.Timeout:
            return "ì‘ë‹µ ì‹œê°„ì´ ì´ˆê³¼ë˜ì—ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
        except Exception as e:
            return f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"


class RAGChatService(BaseChatService):
    """RAG ê¸°ë°˜ íŠ¹í—ˆ ê²€ìƒ‰ ì±—ë´‡ ì„œë¹„ìŠ¤"""

    def __init__(self):
        # OpenAI ì‚¬ìš© ì—¬ë¶€ í™•ì¸
        use_openai = os.getenv('USE_OPENAI_RAG', 'false').lower() == 'true'

        if use_openai:
            from .openai_rag_service import OpenAIRAGService
            self.rag_service = OpenAIRAGService()
            self.use_openai = True
            logger.info("âœ… OpenAI RAG ì„œë¹„ìŠ¤ ì‚¬ìš©")
        else:
            self.rag_service = RAGService()
            self.use_openai = False
            logger.info("âœ… Runpod RAG ì„œë¹„ìŠ¤ ì‚¬ìš©")

        self.model_server_url = getattr(
            settings,
            'MODEL_SERVER_URL',
            'http://localhost:8001'
        )

    def _detect_patent_search(self, message: str) -> bool:
        """íŠ¹í—ˆ ê²€ìƒ‰ ìš”ì²­ì¸ì§€ ê°ì§€"""
        keywords = [
            'íŠ¹í—ˆ', 'ìœ ì‚¬', 'ê²€ìƒ‰', 'ì°¾ì•„', 'ê´€ë ¨',
            'ë¹„ìŠ·í•œ', 'patent', 'similar', 'search'
        ]
        return any(keyword in message.lower() for keyword in keywords)

    def generate_response(self, message: str, file_content: Optional[str] = None,
                         conversation_history: Optional[List[Dict]] = None) -> str:
        """RAGë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ìƒì„±"""

        # ì ì‹¬ ë©”ë‰´ ìš”ì²­ì¸ì§€ ë¨¼ì € í™•ì¸
        is_lunch, category = detect_lunch_request(message)
        if is_lunch:
            if category:
                menus = get_random_menu_by_category(category, 3)
                return f"ì˜¤ëŠ˜ {category} ë©”ë‰´ ì¶”ì²œë“œë¦½ë‹ˆë‹¤:\n" + "\n".join(f"â€¢ {menu}" for menu in menus)
            else:
                menus = get_random_menu(3)
                return f"ì˜¤ëŠ˜ì˜ ì ì‹¬ ë©”ë‰´ ì¶”ì²œë“œë¦½ë‹ˆë‹¤:\n" + "\n".join(f"â€¢ {menu}" for menu in menus)

        # íŠ¹í—ˆ ê²€ìƒ‰ ìš”ì²­ì¸ì§€ í™•ì¸
        if self._detect_patent_search(message):
            # ğŸ”¥ í•˜ë“œì½”ë”© ë°ì´í„° ìš°ì„  ì²´í¬ (CSVì—ì„œ ì§ì ‘ ì°¾ê¸°)
            hardcoded_response = find_matching_claim(message)
            if hardcoded_response:
                logger.info("âœ… í•˜ë“œì½”ë”© ë°ì´í„°ì—ì„œ ë‹µë³€ ì°¾ìŒ")
                return f"ğŸ”´ ê±°ì ˆ íŠ¹í—ˆë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤\n\n{hardcoded_response}"
            try:
                # OpenAI ì‚¬ìš© ì‹œ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ í•œ ë²ˆì— ì²˜ë¦¬
                if self.use_openai:
                    return self.rag_service.rag_pipeline(
                        query=message,
                        use_classification=True,
                        top_k=3
                    )

                # Runpod ì‚¬ìš© ì‹œ: Runpod ì„œë²„ì˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
                # Runpod ì„œë²„ê°€ ë‚´ë¶€ì—ì„œ RAG ê²€ìƒ‰ â†’ ë¶„ë¥˜ â†’ SLLM ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                try:
                    response = requests.post(
                        f"{self.model_server_url}/pipeline",  # âœ… ì˜¬ë°”ë¥¸ ì—”ë“œí¬ì¸íŠ¸
                        json={
                            "query": message,
                            "top_k": 5,  # ìƒìœ„ 5ê°œ íŠ¹í—ˆ ê²€ìƒ‰
                            "use_classification": True,  # ë¶„ë¥˜ ëª¨ë¸ ì‚¬ìš©
                            "max_length": 512
                        },
                        timeout=120  # íŒŒì´í”„ë¼ì¸ì€ ì‹œê°„ì´ ë” ê±¸ë¦´ ìˆ˜ ìˆìŒ
                    )

                    if response.status_code == 200:
                        data = response.json()

                        # ì‘ë‹µ í¬ë§·íŒ…
                        classification = data.get('classification', 'unknown')
                        patents_used = data.get('patents_used', 0)
                        response_text = data.get('response', 'ì‘ë‹µ ìƒì„± ì‹¤íŒ¨')

                        # ë¶„ë¥˜ ê²°ê³¼ í‘œì‹œ
                        if classification == 'rejection':
                            header = f"ğŸ”´ ê±°ì ˆ íŠ¹í—ˆë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤ (ìœ ì‚¬ íŠ¹í—ˆ {patents_used}ê°œ ë¶„ì„)\n\n"
                        elif classification == 'registration':
                            header = f"ğŸŸ¢ ë“±ë¡ íŠ¹í—ˆë¡œ ë¶„ë¥˜ë˜ì—ˆìŠµë‹ˆë‹¤ (ìœ ì‚¬ íŠ¹í—ˆ {patents_used}ê°œ ë¶„ì„)\n\n"
                        else:
                            header = f"ğŸ“Š {patents_used}ê°œì˜ ìœ ì‚¬ íŠ¹í—ˆë¥¼ ë¶„ì„í–ˆìŠµë‹ˆë‹¤\n\n"

                        return header + response_text
                    else:
                        logger.error(f"Runpod íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: HTTP {response.status_code}")
                        return f"ì£„ì†¡í•©ë‹ˆë‹¤. íŠ¹í—ˆ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. (HTTP {response.status_code})"

                except requests.exceptions.RequestException as e:
                    logger.error(f"ëª¨ë¸ ì„œë²„ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ íŠ¹í—ˆ ë¶„ì„ ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            except Exception as e:
                logger.error(f"RAG ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                return f"íŠ¹í—ˆ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"

        # ì¼ë°˜ ì§ˆë¬¸ì€ ê¸°ì¡´ LLaMA ì„œë¹„ìŠ¤ ì‚¬ìš©
        else:
            try:
                # Qwen Chat Template í˜•ì‹ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
                system_msg = "You are a helpful assistant. Answer concisely in Korean. Do not repeat the question or add unnecessary explanations."

                # ëŒ€í™” ê¸°ë¡ í•„í„°ë§
                recent_history = []
                if conversation_history and len(conversation_history) > 0:
                    for msg in conversation_history[-6:]:
                        content = msg.get('content', '')
                        # ì ì‹¬ ë©”ë‰´, ì—ëŸ¬ ë©”ì‹œì§€ ì œì™¸
                        if ('ì ì‹¬' not in content and 'ë©”ë‰´' not in content and
                            not content.startswith('ëª¨ë¸ ì„œë²„ ì˜¤ë¥˜') and
                            not content.startswith('ì˜¤ëŠ˜ì˜') and
                            'ë¨¹' not in content and
                            not content.startswith('ì£„ì†¡í•©ë‹ˆë‹¤')):
                            recent_history.append(msg)
                    recent_history = recent_history[-3:]  # ìµœê·¼ 3ê°œë§Œ

                # Qwen Chat Template êµ¬ì„±
                prompt_parts = [f"<|im_start|>system\n{system_msg}<|im_end|>"]

                # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
                for msg in recent_history:
                    role = "user" if msg['type'] == 'user' else "assistant"
                    prompt_parts.append(f"<|im_start|>{role}\n{msg['content']}<|im_end|>")

                # í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€
                prompt_parts.append(f"<|im_start|>user\n{message}<|im_end|>")
                prompt_parts.append("<|im_start|>assistant")

                prompt = "\n".join(prompt_parts)

                response = requests.post(
                    f"{self.model_server_url}/generate",
                    json={
                        "prompt": prompt,
                        "max_length": 128,
                        "temperature": 0.7
                    },
                    timeout=60
                )

                if response.status_code == 200:
                    data = response.json()
                    return data.get('response', 'ì‘ë‹µ ìƒì„± ì‹¤íŒ¨')
                else:
                    return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."

            except requests.exceptions.RequestException as e:
                logger.error(f"ëª¨ë¸ ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ AI ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."


class ChatServiceFactory:
    """ì±—ë´‡ ì„œë¹„ìŠ¤ íŒ©í† ë¦¬ - ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ ëª¨ë¸ ì„ íƒ"""

    @staticmethod
    def create_service() -> BaseChatService:
        """
        ì„¤ì •ì— ë”°ë¼ ì ì ˆí•œ ì±—ë´‡ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜

        ìš°ì„ ìˆœìœ„:
        1. USE_SIMPLE_OPENAI=true â†’ Simple OpenAI (RAG/ë¶„ë¥˜ ì—†ì´ ìˆœìˆ˜ OpenAIë§Œ)
        2. settings.CHATBOT_SERVICE ê°’ì— ë”°ë¼:
           - 'llama': LLaMA ëª¨ë¸
           - 'custom': ì‚¬ìš©ì ì •ì˜ ëª¨ë¸ (ë©€í‹°í„´ ì§€ì›)
           - 'rag': RAG ê¸°ë°˜ íŠ¹í—ˆ ê²€ìƒ‰ ì±—ë´‡
        """
        # ìµœìš°ì„ : Simple OpenAI ëª¨ë“œ
        if os.getenv('USE_SIMPLE_OPENAI', 'false').lower() == 'true':
            from .simple_openai_service import SimpleOpenAIChatService
            logger.info("âœ… Simple OpenAI ì±—ë´‡ ì„œë¹„ìŠ¤ ì‚¬ìš© (RAG/ë¶„ë¥˜ ì—†ìŒ)")
            return SimpleOpenAIChatService()

        service_type = getattr(settings, 'CHATBOT_SERVICE', 'rag')

        if service_type == 'llama':
            return LlamaChatService()
        elif service_type == 'custom':
            return CustomModelChatService()
        elif service_type == 'rag':
            return RAGChatService()
        else:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ RAG ì‚¬ìš©
            logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì„œë¹„ìŠ¤ íƒ€ì… '{service_type}', RAG ì‚¬ìš©")
            return RAGChatService()


# ì‹±ê¸€í†¤ íŒ¨í„´ìœ¼ë¡œ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ê´€ë¦¬
_chat_service_instance = None

def get_chat_service() -> BaseChatService:
    """ì±—ë´‡ ì„œë¹„ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë°˜í™˜ (ì‹±ê¸€í†¤)"""
    global _chat_service_instance
    if _chat_service_instance is None:
        _chat_service_instance = ChatServiceFactory.create_service()
    return _chat_service_instance
