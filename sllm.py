# -*- coding: utf-8 -*-
"""
Full-dataset inference with a single LoRA checkpoint (checkpoint-16)
- Loads BASE_MODEL + LoRA at CKPT_PATH
- Runs on ALL rows of EVAL_PATH
- Saves one CSV with original cols + pred column
"""

import os, re, json, gc
from typing import List, Dict, Any
from tqdm.auto import tqdm

import torch
import pandas as pd
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ========= Paths =========
BASE_MODEL = "Qwen/Qwen2.5-14B-Instruct"
CKPT_PATH  = "./qwen2.5-14b-qlora-r1/r1/checkpoint-16"   # â† ì—¬ê¸°ë¥¼ checkpoint-16ë¡œ ê³ ì •
EVAL_PATH  = "sllm_pairs_with_gold_filtered.jsonl"
OUT_DIR    = os.path.dirname(CKPT_PATH)
OUT_CSV    = os.path.join(OUT_DIR, "infer_checkpoint-16_full.csv")

# ========= Inference params =========
TOP_K_SIM      = 3
REQ_END        = "ë”°ë¼ì„œ íŠ¹í—ˆë¥¼ ë°›ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
MAX_INPUT_LEN  = 1792
GEN_KW = dict(
    max_new_tokens=256,
    do_sample=False,
    num_beams=3,
    no_repeat_ngram_size=3,
    length_penalty=0.9,
    repetition_penalty=1.1,
)

# ========= Template utils =========
def _clean(x: str) -> str:
    return re.sub(r"\s+", " ", str(x)).strip() if isinstance(x, str) else ""

def _sim_block_and_map(similar_claims: list, k:int=TOP_K_SIM):
    if not isinstance(similar_claims, list) or not similar_claims:
        return "(ìœ ì‚¬ ì²­êµ¬í•­ ì—†ìŒ)", []
    ranked = similar_claims[:k]
    rows, mapping = [], []
    for i, s in enumerate(ranked, 1):
        did = s.get("doc_id","")
        cno = s.get("claim_no","")
        txt = _clean(s.get("text",""))
        rows.append(f"{i}) [{did} / Claim {cno}]\n{txt}")
        mapping.append({"label": f"ì¸ìš©ë°œëª…{i}", "doc_id": did})
    return ("\n\n".join(rows) if rows else "(ìœ ì‚¬ ì²­êµ¬í•­ ì—†ìŒ)"), mapping

SYSTEM_PROMPT = (
    "You are Qwen, a helpful patent analysis assistant.\n"
    "ê·œì¹™:\n"
    "1) ë°˜ë“œì‹œ í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ê³  ì¤‘êµ­ì–´, ì¼ë³¸ì–´ ë“± ì™¸êµ­ì–´(í•œì í¬í•¨)ë¥¼ ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
    "2) ì¶œë ¥ì€ ì¤„ë°”ê¿ˆ ì—†ì´ í•œ ë‹¨ë½ì˜ í•œêµ­ì–´ ê³µì‹ ë¬¸ì¥ìœ¼ë¡œë§Œ ì‘ì„±í•˜ì‹­ì‹œì˜¤.\n"
    "3) ë³¸ë¬¸ì—ì„œ ì¸ìš©ë°œëª…ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ 'ì¸ìš©ë°œëª…N(ì¶œì›ë²ˆí˜¸ XXXXX)' í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ì‹­ì‹œì˜¤.\n"
)

def _mapping_lines(mapping):
    s = "\n".join([f"- {m['label']}: ì¶œì›ë²ˆí˜¸ {m['doc_id']}" for m in mapping if m.get("doc_id")])
    return s if s else "- (ìœ ì‚¬ë¬¸ì„œ ì¶œì›ë²ˆí˜¸ ì—†ìŒ)"

def build_messages_from_row(row: dict, k:int=TOP_K_SIM):
    claim = _clean(row.get("claim_text",""))
    sim_block, mapping = _sim_block_and_map(row.get("similar_claims", []), k=k)
    prior_art_no = row.get("prior_art_no") or row.get("prior_art_code") or row.get("prior")
    prior_line = f"{prior_art_no}" if isinstance(prior_art_no, str) else (str(prior_art_no) if prior_art_no is not None else "")
    user = (
        "ë‹¤ìŒ (ì„ í–‰ë¬¸í—Œ/ìœ ì‚¬ë¬¸ì„œì˜ ì²­êµ¬í•­ ëª©ë¡ê³¼ ëŒ€ìƒ ì²­êµ¬í•­)ì„ ë°”íƒ•ìœ¼ë¡œ, "
        "ê±°ì ˆ ì‚¬ìœ (ì‹ ê·œì„±, ì§„ë³´ì„±, ëª…í™•ì„± ë“±)ë¥¼ íŒë³„í•˜ê³  í•µì‹¬ ê·¼ê±°ë¥¼ 3ì¤„ ì´ë‚´ë¡œ ê°„ê²°íˆ ì„¤ëª…í•´ì¤˜. "
        "ìœ ì‚¬ì ê³¼ ì°¨ì´ì ì„ ëª…í™•íˆ ì§€ì í•´.\n\n"
        f"[ì„ í–‰ë¬¸í—Œ/ì¸ìš© ë²ˆí˜¸]\n{prior_line}\n\n"
        f"[ëŒ€ìƒ ì²­êµ¬í•­]\n{claim}\n\n"
        f"[ìœ ì‚¬ ë¬¸ì„œì˜ ì²­êµ¬í•­ ëª©ë¡ (ìƒìœ„ {k}ê°œ)]\n{sim_block}\n\n"
        "[ì¸ìš©ë°œëª… ë¼ë²¨-ì¶œì›ë²ˆí˜¸ ë§¤í•‘]\n"
        f"{_mapping_lines(mapping)}\n\n"
        "ì£¼ì˜: ë³¸ë¬¸ì—ì„œ ì¸ìš©ë°œëª…ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ë°˜ë“œì‹œ 'ì¸ìš©ë°œëª…N(ì¶œì›ë²ˆí˜¸ XXXXX)' í˜•ì‹ìœ¼ë¡œ í‘œê¸°í•˜ê³ , "
        "í•œêµ­ì–´ë§Œ ì‚¬ìš©í•˜ë©° í•œ ë‹¨ë½ìœ¼ë¡œ ì‘ì„±í•˜ë¼."
    )
    return [
        {"role": "system", "content": SYSTEM_PROMPT},
        {"role": "user",   "content": user},
    ]

# ========= Load model (4-bit + LoRA checkpoint) =========
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True, bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True
)
tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
if tok.pad_token_id is None:
    tok.pad_token = tok.eos_token

base = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True,
    quantization_config=bnb_cfg,
)
model = PeftModel.from_pretrained(base, CKPT_PATH)
model.eval()
print("âœ… Loaded:", CKPT_PATH)

# ========= Load full data =========
rows = []
with open(EVAL_PATH, "r", encoding="utf-8") as f:
    for line in f:
        rows.append(json.loads(line))
print("Samples (full):", len(rows))

# ========= Generate =========
preds, prompt_lens = [], []
for row in tqdm(rows, desc="Generating (checkpoint-16, full)"):
    messages = build_messages_from_row(row, k=TOP_K_SIM)
    prompt = tok.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    enc = tok([prompt], return_tensors="pt", truncation=True, max_length=MAX_INPUT_LEN)
    enc = {k: v.to(model.device) for k, v in enc.items()}

    with torch.inference_mode():
        out = model.generate(**enc, **GEN_KW, pad_token_id=tok.eos_token_id, eos_token_id=tok.eos_token_id)

    input_len = enc["input_ids"].shape[1]
    gen_ids = out[0][input_len:]
    gen_text = tok.decode(gen_ids, skip_special_tokens=True).strip()

    # post-process
    gen_text = re.sub(r"[\u4E00-\u9FFF]+", "", gen_text)
    gen_text = re.sub(r"\s*\n\s*", " ", gen_text).strip()
    if not gen_text.endswith(REQ_END):
        if not gen_text.endswith("."):
            gen_text += "."
        gen_text += f" {REQ_END}"

    preds.append(gen_text)
    prompt_lens.append(input_len)

# ========= Save =========
df_src = pd.DataFrame(rows)
df_src["pred_checkpoint16"] = preds
df_src["prompt_len_checkpoint16"] = prompt_lens
df_src.to_csv(OUT_CSV, index=False, encoding="utf-8-sig")

print(f"ğŸ’¾ Saved: {OUT_CSV}")